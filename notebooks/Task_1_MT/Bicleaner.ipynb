{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34704df-e67f-4a6e-b998-7ada556317d1",
   "metadata": {},
   "source": [
    "This notebook helps you clean your dataset using bicleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48280c39-08cc-4f23-8f87-48141bb19fbc",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86727b6a-bfe5-4d52-ad93-e5dedddd12a1",
   "metadata": {},
   "source": [
    "## 1.1 Before we start, check the operating environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe96ce-1b6f-427e-99fc-d4d5ec6c50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637746d8-fbd1-43e5-964b-b753a49925ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"/pfs/data5/home/kit/stud/u____/myEnv/lib/python3.9/site-packages:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PATH\"] = \"/pfs/data5/home/kit/stud/u____/myEnv/bin:\" + os.environ[\"PATH\"]\n",
    "!which python\n",
    "!which pip\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d4570-f836-4e32-97d2-9e7e2f9c1c3c",
   "metadata": {},
   "source": [
    "## 1.2  Install Bicleaner-AI and download en-de model for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cf6cf-ca52-4d85-96ef-539cf84edaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bicleaner-ai git+https://github.com/MSeal/cython_hunspell@2.0.3\n",
    "!pip install --config-settings=\"--build-option=--max_order=7\" https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b0fab-9b95-4aa2-bff7-bfdd6ebf2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a model for your task, here is en-de\n",
    "!bicleaner-ai-download en de full /pfs/data5/home/kit/stud/u____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd8003-70d6-409f-b8b7-1fe5e56aa379",
   "metadata": {},
   "source": [
    "# 2. Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbb2de-ba1b-40d9-8515-e1f82bab6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD\n",
    "%cd dataset/sample_data\n",
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5883d0-d7b7-4154-b4d5-a9f08db6c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"News-Commentary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbd3d1-158f-457a-9ea5-29119d94a128",
   "metadata": {},
   "source": [
    "## 2.1 Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f117e0f-919d-4bf8-bab4-b97129562f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parallel_corpora(german_file, english_file, output_file):\n",
    "    with open(german_file, 'r', encoding='utf-8') as g_file, \\\n",
    "         open(english_file, 'r', encoding='utf-8') as e_file, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        \n",
    "        for german_line, english_line in zip(g_file, e_file):\n",
    "            merged_line = f\"{english_line.strip()}\\t{german_line.strip()}\\n\"\n",
    "            out_file.write(merged_line)\n",
    "\n",
    "german_file_path = f'{name}.de-en.de'\n",
    "english_file_path = f'{name}.de-en.en'\n",
    "output_file_path = 'merged.txt'\n",
    "\n",
    "merge_parallel_corpora(german_file_path, english_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8ac68-ef26-467d-915d-03fa48f7aeae",
   "metadata": {},
   "source": [
    "## 2.2 Apply Bicleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e723d5-3155-4faa-bef1-3cf253d94163",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PWD\n",
    "!bicleaner-ai-classify --scol 1 --tcol 2 merged.txt result.txt bicleaner-models\n",
    "# 2000 Years later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa8295-3c4d-44f6-8c61-e3c331bded3d",
   "metadata": {},
   "source": [
    "## 2.3 Split into two documents as original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db618fc-d542-43b4-989f-b2f97b42963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter German and English sentences based on score threshold\n",
    "score_threshold = 0.9\n",
    "\n",
    "def filter_corpora_and_output_separate_files(input_file, german_output_file, english_output_file, threshold):\n",
    "    with open(input_file, 'r', encoding='utf-8') as in_file, \\\n",
    "         open(german_output_file, 'w', encoding='utf-8') as g_out_file, \\\n",
    "         open(english_output_file, 'w', encoding='utf-8') as e_out_file:\n",
    "        \n",
    "        for line in in_file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            \n",
    "            english_sentence = parts[0]\n",
    "            german_sentence = parts[1]\n",
    "            score = float(parts[2])\n",
    "            \n",
    "            if score >= threshold:\n",
    "                g_out_file.write(f\"{german_sentence}\\n\")\n",
    "                e_out_file.write(f\"{english_sentence}\\n\")\n",
    "\n",
    "input_file_path = 'result.txt'\n",
    "german_output_file_path = f'bicleaner.{name}.de-en.de'\n",
    "english_output_file_path = f'bicleaner.{name}.de-en.en'\n",
    "\n",
    "filter_corpora_and_output_separate_files(input_file_path, german_output_file_path, english_output_file_path, score_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9c7ff-4a7b-4796-9547-05eff127198c",
   "metadata": {},
   "source": [
    "## 2.4 Delete Intermediate Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9522eaa-9301-4027-b821-bfe3a46a5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afacabdc-3e27-436f-83cf-26a4baa62aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc5b75-3c36-4df0-ae5c-90a0a6d50a97",
   "metadata": {},
   "source": [
    "## 2.5 Split into train/dev/tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afd1c5-7807-41f8-9b93-ba4176071ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_parallel_datasets(german_file_path, english_file_path, german_output_paths, english_output_paths):\n",
    "    filtered_lines = []\n",
    "    with open(german_file_path, encoding=\"utf-8\") as german_file, open(english_file_path, encoding=\"utf-8\") as english_file:\n",
    "        for german_line, english_line in zip(german_file, english_file):\n",
    "            filtered_lines.append((german_line, english_line))\n",
    "            # # If want to delete sentences longer than 25\n",
    "            # if german_line.strip() and english_line.strip() and len(german_line.split()) <= 25:\n",
    "            #     filtered_lines.append((german_line, english_line))\n",
    "\n",
    "    print(f\"Filtered Dataset size: {len(filtered_lines)}\")\n",
    "\n",
    "    # 96:2:2\n",
    "    total_samples = len(filtered_lines)\n",
    "    train_end = int(total_samples * 0.96)\n",
    "    test_end = int(total_samples * 0.98)\n",
    "    splits = {\n",
    "        \"train\": filtered_lines[:train_end],\n",
    "        \"test\": filtered_lines[train_end:test_end],\n",
    "        \"dev\": filtered_lines[test_end:],\n",
    "    }\n",
    "\n",
    "    for split_name, split_data in splits.items():\n",
    "        german_output_dir = os.path.dirname(german_output_paths[split_name])\n",
    "        english_output_dir = os.path.dirname(english_output_paths[split_name])\n",
    "        os.makedirs(german_output_dir, exist_ok=True)\n",
    "        os.makedirs(english_output_dir, exist_ok=True)\n",
    "\n",
    "        with open(german_output_paths[split_name], \"w\", encoding=\"utf-8\") as german_output, \\\n",
    "            open(english_output_paths[split_name], \"w\", encoding=\"utf-8\") as english_output:\n",
    "            for german_line, english_line in split_data:\n",
    "                german_output.write(german_line)\n",
    "                english_output.write(english_line)\n",
    "\n",
    "\n",
    "german_file_path = f\"bicleaner.{name}.de-en.de\"\n",
    "english_file_path = f\"bicleaner.{name}.de-en.en\"\n",
    "german_output_paths = {\n",
    "    key: f\"bicleaner.{name}/{split}.de-en.de\"\n",
    "    for key, split in {\"train\": \"train\", \"test\": \"tst\", \"dev\": \"dev\"}.items()\n",
    "}\n",
    "english_output_paths = {\n",
    "    key: f\"bicleaner.{name}/{split}.de-en.en\"\n",
    "    for key, split in {\"train\": \"train\", \"test\": \"tst\", \"dev\": \"dev\"}.items()\n",
    "}\n",
    "\n",
    "split_parallel_datasets(\n",
    "    german_file_path, english_file_path, german_output_paths, english_output_paths\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a3702-5658-4dd3-bf72-fc600d02d251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
