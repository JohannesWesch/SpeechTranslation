{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.executable should point to your virtualenv's Python interpreter.\n",
    "# sys.path should include your virtualenv's site-packages directory.\n",
    "# print(sys.executable)\n",
    "# print(sys.path)\n",
    "# !echo \"\"\n",
    "import os\n",
    "# update PYTHONPATH and PATH\n",
    "os.environ[\"PYTHONPATH\"] = \"/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PATH\"] = \"/pfs/data5/home/kit/stud/unyfv/myEnv/bin:\" + os.environ[\"PATH\"]\n",
    "# # check\n",
    "# !which python\n",
    "# !which pip\n",
    "# !echo \"\"\n",
    "!echo $PYTHONPATH\n",
    "os.environ['PYTHONPATH'] += \":/pfs/data5/home/kit/stud/u/fairseq/\"\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Let's fetch some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root_dir = Path(\"/pfs/data5/home/kit/stud/unyfv/cv-corpus-19.0-2024-09-13/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchaudio.datasets import COMMONVOICE\n",
    "# If you got 'an undefined symbol' error, check your torch_version.\n",
    "# my_torch_version == 2.1.0 and my_torchaudio_version == 2.1.0\n",
    "\n",
    "train_data = COMMONVOICE(root_dir, tsv=\"train.tsv\") # len = 595998\n",
    "dev_data = COMMONVOICE(root_dir, tsv=\"dev.tsv\") # len = 16188\n",
    "test_data = COMMONVOICE(root_dir, tsv=\"test.tsv\") # len = 16188\n",
    "\n",
    "# Sneak peek at the data:\n",
    "print(train_data[0])\n",
    "# If you got 'Couldn't find appropriate backend to handle uri *** and format None' error\n",
    "# Unfortunately I forgot how to fix this error... xd\n",
    "# But I know what I tried:\n",
    "# pip install ffmpeg pysoundfile librosa soundfile\n",
    "# Install them all and restart your kernel... Wooow, amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extract log mel filter bank features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from fairseq.examples.speech_to_text.data_utils import (\n",
    "    create_zip,\n",
    "    extract_fbank_features,\n",
    "    gen_config_yaml,\n",
    "    gen_vocab,\n",
    "    get_zip_manifest,\n",
    "    save_df_to_tsv,\n",
    ")\n",
    "\n",
    "# Define file path to store the features\n",
    "feature_root = root_dir / \"fbank80\"\n",
    "feature_root.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract features of all audios in all data splits\n",
    "for dataset in [train_data, dev_data, test_data]:\n",
    "    print(f\"Extracting log mel filter bank features of audio\")\n",
    "    for wav, sample_rate, metadata in tqdm(dataset):\n",
    "        sample_id = f\"{metadata['client_id']}-{metadata['sentence_id']}\"\n",
    "        extract_fbank_features(\n",
    "                wav, sample_rate, feature_root / f\"{sample_id}.npy\"\n",
    "        )\n",
    "\n",
    "# Pack audio features into ZIP\n",
    "zip_path = root_dir / \"fbank80.zip\"\n",
    "print(\"ZIPing features...\")\n",
    "create_zip(feature_root, zip_path) # len = 628374"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generating TSV (Tab Separated Values) manifest files, containing samples' metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching audio manifest...\")\n",
    "audio_paths, audio_lengths = get_zip_manifest(zip_path)\n",
    "\n",
    "import pickle\n",
    "with open(root_dir / 'audio_manifest.pkl', 'wb') as f:\n",
    "    pickle.dump((audio_paths, audio_lengths), f)\n",
    "print(\"Saved as audio_manifest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open(root_dir / 'audio_manifest.pkl', 'rb') as file:\n",
    "    audio_paths, audio_lengths = pickle.load(file)\n",
    "\n",
    "MANIFEST_COLUMNS = [\"id\", \"audio\", \"n_frames\", \"tgt_text\", \"speaker\"]\n",
    "# for dataset, split_name in zip([train_data, dev_data, test_data],[\"train-clean\", \"dev-clean\", \"test-clean\"]):\n",
    "for dataset, split_name in zip([dev_data, test_data],[\"dev-clean\", \"test-clean\"]):\n",
    "    print(f\"Fetching manifest from {split_name}...\")\n",
    "    manifest = {c: [] for c in MANIFEST_COLUMNS}\n",
    "    for _, _, metadata in tqdm(dataset):\n",
    "        sample_id = f\"{metadata['client_id']}-{metadata['sentence_id']}\"\n",
    "        manifest[\"id\"].append(sample_id)\n",
    "        manifest[\"audio\"].append(audio_paths[sample_id])\n",
    "        manifest[\"n_frames\"].append(audio_lengths[sample_id])\n",
    "        manifest[\"tgt_text\"].append(metadata['sentence'])\n",
    "        manifest[\"speaker\"].append(metadata['client_id'])\n",
    "    \n",
    "    save_df_to_tsv(pd.DataFrame.from_dict(manifest), root_dir / f\"{split_name}.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 For text data, we use the [sentencepiece](https://github.com/google/sentencepiece) package to train a subwords segmentation model and generate the subword vocabulary (same as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect train text to generate sentencepiece model and vocabulary later on\n",
    "train_text = pd.read_csv(root_dir / \"train-clean.tsv\", sep='\\t')[\"tgt_text\"].tolist()\n",
    "with open(root_dir / 'train_text.txt', 'w') as f:\n",
    "    for t in train_text:\n",
    "        f.write(t + \"\\n\")\n",
    "\n",
    "# Train sentencepiece model and generate subword vocabulary\n",
    "# The vocab size depends on your dataset size.\n",
    "VOCAB_SIZE = 50000\n",
    "gen_vocab(\n",
    "    Path(root_dir / 'train_text.txt'),\n",
    "    root_dir / f\"spm_unigram{VOCAB_SIZE}\",\n",
    "    model_type='unigram',\n",
    "    vocab_size=VOCAB_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Generate config YAML file to be used for the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config_yaml(\n",
    "    root_dir,\n",
    "    spm_filename=f\"spm_unigram{VOCAB_SIZE}.model\"\n",
    ")\n",
    "# This file is config.yaml in root_dir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the left menu bar 'softwares' load cudnn/9.2 if you meet an error about cudnn.\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train \"/pfs/data5/home/kit/stud/unyfv/cv-corpus-19.0-2024-09-13/de\" \\\n",
    "  --save-dir \"/pfs/data5/home/kit/stud/unyfv/cv-corpus-19.0-2024-09-13/de/models\" \\\n",
    "  --train-subset train-clean --valid-subset dev-clean \\\n",
    "  --num-workers 8 --max-tokens 40000 --max-update 300000 \\\n",
    "  --task speech_to_text --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --report-accuracy \\\n",
    "  --arch s2t_transformer_s --share-decoder-input-output-embed \\\n",
    "  --optimizer adam --lr 2e-3 --lr-scheduler inverse_sqrt --warmup-updates 10000 \\\n",
    "  --clip-norm 10.0 --seed 1 --update-freq 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference & Evaluation\n",
    "\n",
    "Now we can generate transcription with the trained model. Notice that we set the `scoring` parameter to be `wer`. WER, i.e., **Word Error Rate**, is the metrics that we use to evaluate our English ASR model. WER is defined as the number of errornously transcribed words divided by the total number of words in the reference sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install editdistance\n",
    "PRED_OUTPUT_DIR = \"/pfs/data5/home/kit/stud/unyfv/cv-corpus-19.0-2024-09-13/de/pred_eval\"\n",
    "PRED_LOG = f\"{PRED_OUTPUT_DIR}/en_s2t.pred.log\"\n",
    "!mkdir $PRED_OUTPUT_DIR\n",
    "!fairseq-generate \"/pfs/data5/home/kit/stud/unyfv/cv-corpus-19.0-2024-09-13/de\" \\\n",
    "    --config-yaml config.yaml --gen-subset test-clean \\\n",
    "    --task speech_to_text \\\n",
    "    --path \"/pfs/data5/home/kit/stud/unyfv/cv-corpus-19.0-2024-09-13/de/models/checkpoint_best.pt\" \\\n",
    "    --max-tokens 50000 --beam 5 --scoring wer > $PRED_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep ^D $PRED_LOG | sed 's/^D-//g' | cut -f 3 | sed 's/ ##//g' > $PRED_OUTPUT_DIR/hyp.txt\n",
    "!grep ^T $PRED_LOG | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > $PRED_OUTPUT_DIR/ref.txt\n",
    "!head $PRED_OUTPUT_DIR/hyp.txt\n",
    "!echo \"\"\n",
    "!head $PRED_OUTPUT_DIR/ref.txt\n",
    "!echo \"\"\n",
    "!tail -n 1 $PRED_LOG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
