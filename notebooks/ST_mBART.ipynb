{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install pip and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('compiler/gnu/13.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv/myEnv/bin/python\n",
      "['/pfs/data5/home/kit/stud/unyfv', '', '/home/kit/stud/unyfv/.local/lib/python3.9/site-packages', '/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages', '/usr/lib64/python39.zip', '/usr/lib64/python3.9', '/usr/lib64/python3.9/lib-dynload', '/pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages', '/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.executable 应该指向你的虚拟环境中的 Python 解释器。\n",
    "# sys.path 中应该包含你虚拟环境的 site-packages 目录。\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv/myEnv/bin/python\n",
      "/pfs/data5/home/kit/stud/unyfv/myEnv/bin/pip\n",
      "/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:/home/kit/stud/unyfv/.local/lib/python3.9/site-packages:/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 更新 PYTHONPATH 和 PATH\n",
    "os.environ[\"PYTHONPATH\"] = \"/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PATH\"] = \"/pfs/data5/home/kit/stud/unyfv/myEnv/bin:\" + os.environ[\"PATH\"]\n",
    "# 验证更新\n",
    "!which python\n",
    "!which pip\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip==24.0 in ./myEnv/lib/python3.9/site-packages (24.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pip==24.0\n",
    "!pip show torch | grep Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Install fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install fairseq, trust me, the original installation method sucks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/fairseq.git\n",
    "%cd fairseq\n",
    "!pip install --editable ./ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to add a new environment variable so that we can use the fairseq command in the terminal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:/home/kit/stud/unyfv/.local/lib/python3.9/site-packages:/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages\n",
      "/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:/home/kit/stud/unyfv/.local/lib/python3.9/site-packages:/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages:/pfs/data5/home/kit/stud/unyfv/fairseq/\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH\n",
    "os.environ['PYTHONPATH'] += \":/pfs/data5/home/kit/stud/unyfv/fairseq/\"\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Install other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses\n",
    "!pip install sentencepiece\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Activate GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "Current device:  Tesla V100-SXM2-32GB\n",
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device(); print('Current device: ', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = 'cpu'; print('Current device: CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all packages have been installed.\n",
    "From now on, just execute the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we still use TED-dataset as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O sample_data.zip https://bwsyncandshare.kit.edu/s/Xx3D56SJmG8PwXj/download\n",
    "!unzip sample_data.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment the text into subwords using BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# After execution, you can find two bpe files in the directory.\n",
    "spm.SentencePieceTrainer.train(input=\"dataset/sample_data/train.de-en.en,dataset/sample_data/train.de-en.de\",\n",
    "                               model_prefix=\"bpe\",\n",
    "                               vocab_size=10000)\n",
    "\n",
    "print('Finished training sentencepiece model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the trained segmentation model to preprocess the sentences from train/dev/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained sentencepiece model\n",
    "spm_model = spm.SentencePieceProcessor(model_file=\"bpe.model\")\n",
    "\n",
    "# Important: encoding=\"utf-8\"\n",
    "for partition in [\"train\", \"dev\", \"tst\"]:\n",
    "    for lang in [\"de\", \"en\"]:\n",
    "        f_out = open(f\"dataset/sample_data/spm.{partition}.de-en.{lang}\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "        with open(f\"dataset/sample_data/{partition}.de-en.{lang}\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "            for line_idx, line in enumerate(f_in.readlines()):\n",
    "                # Segmented into subwords\n",
    "                line_segmented = spm_model.encode(line.strip(), out_type=str)\n",
    "                # Join the subwords into a string\n",
    "                line_segmented = \" \".join(line_segmented)\n",
    "                f_out.write(line_segmented + \"\\n\")\n",
    "\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will binarize the data for training with fairseq.  \n",
    "Feel free to check the [documentation](https://fairseq.readthedocs.io/en/latest/command_line_tools.html) of fairseq commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "# Preprocess/binarize the data\n",
    "TEXT=\"dataset/sample_data\"\n",
    "!echo $TEXT\n",
    "# Binarize the data for training\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de --target-lang en \\\n",
    "    --trainpref $TEXT/spm.train.de-en \\\n",
    "    --validpref $TEXT/spm.dev.de-en \\\n",
    "    --testpref $TEXT/spm.tst.de-en \\\n",
    "    --destdir binarized_data/iwslt14.de-en \\\n",
    "    --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/sample_data\n",
      "2024-11-01 20:31:03.455656: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 20:31:03.491655: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 20:31:03.491690: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 20:31:03.491720: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 20:31:03.499411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 20:31:04.861621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='dataset/sample_data/spm.train.de-en', validpref='dataset/sample_data/spm.dev.de-en', testpref='dataset/sample_data/spm.tst.de-en', align_suffix=None, destdir='binarized_data/iwslt14.de-en', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=8, dict_only=False)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 10128 types\n",
      "INFO:fairseq_cli.preprocess:[de] dataset/sample_data/spm.train.de-en.de: 200592 sents, 3584781 tokens, 0.0% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 10128 types\n",
      "INFO:fairseq_cli.preprocess:[de] dataset/sample_data/spm.dev.de-en.de: 4179 sents, 74522 tokens, 0.00134% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 10128 types\n",
      "INFO:fairseq_cli.preprocess:[de] dataset/sample_data/spm.tst.de-en.de: 4179 sents, 73200 tokens, 0.00137% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 10128 types\n",
      "INFO:fairseq_cli.preprocess:[en] dataset/sample_data/spm.train.de-en.en: 200592 sents, 3665856 tokens, 0.0% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 10128 types\n",
      "INFO:fairseq_cli.preprocess:[en] dataset/sample_data/spm.dev.de-en.en: 4179 sents, 74674 tokens, 0.0% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 10128 types\n",
      "INFO:fairseq_cli.preprocess:[en] dataset/sample_data/spm.tst.de-en.en: 4179 sents, 72921 tokens, 0.00137% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:Wrote preprocessed data to binarized_data/iwslt14.de-en\n"
     ]
    }
   ],
   "source": [
    "# mBART https://github.com/facebookresearch/fairseq/blob/main/examples/mbart/README.md\n",
    "# Preprocess/binarize the data\n",
    "TEXT=\"dataset/sample_data\"\n",
    "!echo $TEXT\n",
    "# Binarize the data for training\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang de --target-lang en \\\n",
    "    --trainpref $TEXT/spm.train.de-en \\\n",
    "    --validpref $TEXT/spm.dev.de-en \\\n",
    "    --testpref $TEXT/spm.tst.de-en \\\n",
    "    --destdir binarized_data/iwslt14.de-en \\\n",
    "    --joined-dictionary \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 20:32:03.253714: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 20:32:03.290233: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 20:32:03.290269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 20:32:03.290292: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 20:32:03.297900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 20:32:04.386201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-01 20:32:05 | INFO | numexpr.utils | Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2024-11-01 20:32:05 | INFO | numexpr.utils | Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-11-01 20:32:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-11-01 20:32:09 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='mbart_large', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='binarized_data/iwslt14.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, no_scale_embedding=False, encoder_embed_path=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_attention_heads=16, encoder_normalize_before=False, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=12, decoder_attention_heads=16, decoder_normalize_before=False, decoder_learned_pos=True, attention_dropout=0.0, relu_dropout=0.0, max_target_positions=1024, max_source_positions=1024, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=True, decoder_output_dim=1024, decoder_input_dim=1024, layernorm_embedding=True, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0, _name='mbart_large'), 'task': {'_name': 'translation', 'data': 'binarized_data/iwslt14.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2024-11-01 20:32:09 | INFO | fairseq.tasks.translation | [de] dictionary: 10128 types\n",
      "2024-11-01 20:32:09 | INFO | fairseq.tasks.translation | [en] dictionary: 10128 types\n",
      "2024-11-01 20:32:14 | INFO | fairseq_cli.train | BARTModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(10128, 1024, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(10128, 1024, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=1024, out_features=10128, bias=False)\n",
      "  )\n",
      "  (classification_heads): ModuleDict()\n",
      ")\n",
      "2024-11-01 20:32:14 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2024-11-01 20:32:14 | INFO | fairseq_cli.train | model: BARTModel\n",
      "2024-11-01 20:32:14 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2024-11-01 20:32:14 | INFO | fairseq_cli.train | num. shared model params: 365,191,168 (num. trained: 365,191,168)\n",
      "2024-11-01 20:32:14 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2024-11-01 20:32:14 | INFO | fairseq.data.data_utils | loaded 4,179 examples from: binarized_data/iwslt14.de-en/valid.de-en.de\n",
      "2024-11-01 20:32:14 | INFO | fairseq.data.data_utils | loaded 4,179 examples from: binarized_data/iwslt14.de-en/valid.de-en.en\n",
      "2024-11-01 20:32:14 | INFO | fairseq.tasks.translation | binarized_data/iwslt14.de-en valid de-en 4179 examples\n",
      "2024-11-01 20:32:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2024-11-01 20:32:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2024-11-01 20:32:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2024-11-01 20:32:16 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.733 GB ; name = Tesla V100-SXM2-32GB                    \n",
      "2024-11-01 20:32:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2024-11-01 20:32:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2024-11-01 20:32:16 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2024-11-01 20:32:16 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "2024-11-01 20:32:16 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
      "2024-11-01 20:32:16 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2024-11-01 20:32:16 | INFO | fairseq.data.data_utils | loaded 200,592 examples from: binarized_data/iwslt14.de-en/train.de-en.de\n",
      "2024-11-01 20:32:16 | INFO | fairseq.data.data_utils | loaded 200,592 examples from: binarized_data/iwslt14.de-en/train.de-en.en\n",
      "2024-11-01 20:32:16 | INFO | fairseq.tasks.translation | binarized_data/iwslt14.de-en train de-en 200592 examples\n",
      "2024-11-01 20:32:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:32:16 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-11-01 20:32:16 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-11-01 20:32:16 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2024-11-01 20:32:17 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2024-11-01 20:32:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:32:17 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-11-01 20:32:17 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-11-01 20:32:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2024-11-01 20:32:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 001:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:32:18 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2024-11-01 20:32:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2024-11-01 20:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   2%|▌                             | 20/1011 [00:12<04:07,  4.00it/s]2024-11-01 20:32:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001: 100%|▉| 1010/1011 [04:15<00:00,  4.11it/s, loss=7.462, nll_loss=6.5292024-11-01 20:36:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 20:36:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 17.46it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 18.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.01it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 17.85it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:01<00:00, 17.88it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.29it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.50it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 20:36:35 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.269 | nll_loss 6.266 | ppl 76.98 | wps 38470.3 | wpb 2192.2 | bsz 122.8 | num_updates 1009\n",
      "2024-11-01 20:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1009 updates\n",
      "2024-11-01 20:36:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint1.pt\n",
      "2024-11-01 20:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint1.pt\n",
      "2024-11-01 20:36:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 1009 updates, score 7.269) (writing took 20.99626315291971 seconds)\n",
      "2024-11-01 20:36:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2024-11-01 20:36:56 | INFO | train | epoch 001 | loss 8.589 | nll_loss 7.853 | ppl 231.24 | wps 13483.6 | ups 3.72 | wpb 3625.6 | bsz 198 | num_updates 1009 | lr 0.000126125 | gnorm 1.747 | loss_scale 32 | train_wall 252 | gb_free 21.2 | wall 280\n",
      "2024-11-01 20:36:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:37:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 002:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:37:03 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2024-11-01 20:37:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002: 100%|▉| 1010/1011 [04:11<00:00,  4.05it/s, loss=6.607, nll_loss=5.5442024-11-01 20:41:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 20:41:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.58it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.42it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.59it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.16it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.58it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.91it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.43it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.37it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:01<00:00, 17.81it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.65it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.19it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.76it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.77it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.45it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.65it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 20:41:17 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.284 | nll_loss 5.087 | ppl 33.99 | wps 38718.2 | wpb 2192.2 | bsz 122.8 | num_updates 2020 | best_loss 6.284\n",
      "2024-11-01 20:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2020 updates\n",
      "2024-11-01 20:41:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint2.pt\n",
      "2024-11-01 20:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint2.pt\n",
      "2024-11-01 20:41:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 2020 updates, score 6.284) (writing took 22.071919904090464 seconds)\n",
      "2024-11-01 20:41:39 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2024-11-01 20:41:39 | INFO | train | epoch 002 | loss 6.855 | nll_loss 5.83 | ppl 56.89 | wps 12940.1 | ups 3.57 | wpb 3626 | bsz 198.4 | num_updates 2020 | lr 0.0002525 | gnorm 1.114 | loss_scale 32 | train_wall 246 | gb_free 20.7 | wall 563\n",
      "2024-11-01 20:41:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:41:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 003:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:41:40 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2024-11-01 20:41:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  53%|▌| 533/1011 [02:15<01:58,  4.03it/s, loss=6.05, nll_loss=4.901, 2024-11-01 20:43:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003:  98%|▉| 993/1011 [04:08<00:04,  4.05it/s, loss=7.202, nll_loss=6.209,2024-11-01 20:45:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 003: 100%|▉| 1010/1011 [04:12<00:00,  4.09it/s, loss=7.202, nll_loss=6.2092024-11-01 20:45:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 20:45:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.36it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.33it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.53it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.15it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.59it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.52it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:01<00:00, 17.64it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.53it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.18it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.77it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.81it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.70it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 20:45:54 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.357 | nll_loss 6.292 | ppl 78.35 | wps 38763.3 | wpb 2192.2 | bsz 122.8 | num_updates 3029 | best_loss 6.284\n",
      "2024-11-01 20:45:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3029 updates\n",
      "2024-11-01 20:45:54 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint3.pt\n",
      "2024-11-01 20:46:00 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint3.pt\n",
      "2024-11-01 20:46:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 3029 updates, score 7.357) (writing took 13.959799823351204 seconds)\n",
      "2024-11-01 20:46:08 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2024-11-01 20:46:08 | INFO | train | epoch 003 | loss 6.739 | nll_loss 5.683 | ppl 51.38 | wps 13611.4 | ups 3.75 | wpb 3625.4 | bsz 198.4 | num_updates 3029 | lr 0.000378625 | gnorm 1.582 | loss_scale 8 | train_wall 249 | gb_free 20.5 | wall 832\n",
      "2024-11-01 20:46:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:46:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 004:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:46:08 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2024-11-01 20:46:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 1010/1011 [04:10<00:00,  4.30it/s, loss=7.181, nll_loss=6.1912024-11-01 20:50:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 20:50:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.49it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.38it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.59it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.17it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.58it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.48it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:01<00:00, 17.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.79it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.33it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.87it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.87it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.53it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.73it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.61it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 20:50:21 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.323 | nll_loss 6.212 | ppl 74.12 | wps 38882.1 | wpb 2192.2 | bsz 122.8 | num_updates 4040 | best_loss 6.284\n",
      "2024-11-01 20:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4040 updates\n",
      "2024-11-01 20:50:21 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint4.pt\n",
      "2024-11-01 20:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint4.pt\n",
      "2024-11-01 20:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 4040 updates, score 7.323) (writing took 13.859416137915105 seconds)\n",
      "2024-11-01 20:50:35 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2024-11-01 20:50:35 | INFO | train | epoch 004 | loss 7.147 | nll_loss 6.151 | ppl 71.06 | wps 13745.1 | ups 3.79 | wpb 3626 | bsz 198.4 | num_updates 4040 | lr 0.000497519 | gnorm 1.243 | loss_scale 8 | train_wall 248 | gb_free 21.2 | wall 1099\n",
      "2024-11-01 20:50:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:50:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 005:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:50:38 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2024-11-01 20:50:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 1010/1011 [04:08<00:00,  4.15it/s, loss=7.065, nll_loss=6.06,2024-11-01 20:54:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 20:54:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.64it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.50it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.24it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.64it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 18.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.56it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.53it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:00<00:00, 17.99it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.84it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.37it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.55it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.62it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 20:54:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.793 | nll_loss 6.743 | ppl 107.15 | wps 38966.6 | wpb 2192.2 | bsz 122.8 | num_updates 5051 | best_loss 6.284\n",
      "2024-11-01 20:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5051 updates\n",
      "2024-11-01 20:54:49 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint5.pt\n",
      "2024-11-01 20:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint5.pt\n",
      "2024-11-01 20:55:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 5051 updates, score 7.793) (writing took 13.979498657863587 seconds)\n",
      "2024-11-01 20:55:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2024-11-01 20:55:03 | INFO | train | epoch 005 | loss 7.081 | nll_loss 6.078 | ppl 67.53 | wps 13663.3 | ups 3.77 | wpb 3626 | bsz 198.4 | num_updates 5051 | lr 0.00044495 | gnorm 1.244 | loss_scale 8 | train_wall 244 | gb_free 20.5 | wall 1367\n",
      "2024-11-01 20:55:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:55:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 006:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:55:03 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2024-11-01 20:55:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 1010/1011 [04:07<00:00,  4.09it/s, loss=6.96, nll_loss=5.941,2024-11-01 20:59:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 20:59:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.60it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.49it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.67it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.23it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.63it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.97it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.53it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.50it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:00<00:00, 17.96it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.81it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.35it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.89it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.89it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.55it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.75it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 20:59:13 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.745 | nll_loss 6.721 | ppl 105.47 | wps 38923.3 | wpb 2192.2 | bsz 122.8 | num_updates 6062 | best_loss 6.284\n",
      "2024-11-01 20:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6062 updates\n",
      "2024-11-01 20:59:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint6.pt\n",
      "2024-11-01 20:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint6.pt\n",
      "2024-11-01 20:59:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 6062 updates, score 7.745) (writing took 13.67664478207007 seconds)\n",
      "2024-11-01 20:59:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2024-11-01 20:59:27 | INFO | train | epoch 006 | loss 6.96 | nll_loss 5.941 | ppl 61.43 | wps 13890.5 | ups 3.83 | wpb 3626 | bsz 198.4 | num_updates 6062 | lr 0.000406155 | gnorm 1.494 | loss_scale 8 | train_wall 243 | gb_free 20.7 | wall 1631\n",
      "2024-11-01 20:59:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 20:59:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 007:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 20:59:29 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2024-11-01 20:59:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 1010/1011 [04:11<00:00,  4.08it/s, loss=6.86, nll_loss=5.827,2024-11-01 21:03:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 21:03:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:02, 13.30it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 16.22it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 16.86it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 17.15it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 16.97it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.55it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.32it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:01<00:00, 17.84it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.74it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.30it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.86it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.86it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.51it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.71it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 21:03:44 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.643 | nll_loss 6.541 | ppl 93.1 | wps 38918.1 | wpb 2192.2 | bsz 122.8 | num_updates 7073 | best_loss 6.284\n",
      "2024-11-01 21:03:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7073 updates\n",
      "2024-11-01 21:03:44 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint7.pt\n",
      "2024-11-01 21:03:49 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint7.pt\n",
      "2024-11-01 21:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 7073 updates, score 7.643) (writing took 13.999426000751555 seconds)\n",
      "2024-11-01 21:03:58 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2024-11-01 21:03:58 | INFO | train | epoch 007 | loss 6.886 | nll_loss 5.857 | ppl 57.96 | wps 13526.3 | ups 3.73 | wpb 3626 | bsz 198.4 | num_updates 7073 | lr 0.000376009 | gnorm 2.253 | loss_scale 8 | train_wall 248 | gb_free 21.1 | wall 1902\n",
      "2024-11-01 21:03:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 21:03:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 008:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 21:03:58 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2024-11-01 21:03:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  55%|▌| 560/1011 [02:16<01:52,  3.99it/s, loss=6.835, nll_loss=5.798,2024-11-01 21:06:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 008: 100%|▉| 1010/1011 [04:05<00:00,  4.01it/s, loss=6.893, nll_loss=5.8622024-11-01 21:08:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 21:08:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.55it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.47it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.64it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.23it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.65it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 17.93it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.48it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.48it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:00<00:00, 17.98it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.84it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.38it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.92it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.92it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.57it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.77it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 21:08:06 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.174 | nll_loss 7.215 | ppl 148.61 | wps 38969.1 | wpb 2192.2 | bsz 122.8 | num_updates 8083 | best_loss 6.284\n",
      "2024-11-01 21:08:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8083 updates\n",
      "2024-11-01 21:08:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint8.pt\n",
      "2024-11-01 21:08:12 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint8.pt\n",
      "2024-11-01 21:08:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 8083 updates, score 8.174) (writing took 13.83742597233504 seconds)\n",
      "2024-11-01 21:08:20 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2024-11-01 21:08:20 | INFO | train | epoch 008 | loss 6.855 | nll_loss 5.821 | ppl 56.53 | wps 13977.7 | ups 3.86 | wpb 3625.6 | bsz 198.5 | num_updates 8083 | lr 0.000351733 | gnorm 3.894 | loss_scale 4 | train_wall 243 | gb_free 20.2 | wall 2164\n",
      "2024-11-01 21:08:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 21:08:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 009:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 21:08:24 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2024-11-01 21:08:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:  61%|▌| 615/1011 [02:35<01:37,  4.07it/s, loss=6.895, nll_loss=5.866,2024-11-01 21:10:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "epoch 009: 100%|▉| 1010/1011 [04:11<00:00,  4.06it/s, loss=6.976, nll_loss=5.9572024-11-01 21:12:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 21:12:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.52it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.48it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.66it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.24it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.66it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 18.03it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.59it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.55it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:00<00:00, 18.02it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.87it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.42it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.94it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.94it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.58it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.77it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 21:12:37 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.431 | nll_loss 6.357 | ppl 81.97 | wps 39048.5 | wpb 2192.2 | bsz 122.8 | num_updates 9093 | best_loss 6.284\n",
      "2024-11-01 21:12:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9093 updates\n",
      "2024-11-01 21:12:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint9.pt\n",
      "2024-11-01 21:12:42 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint9.pt\n",
      "2024-11-01 21:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 9093 updates, score 7.431) (writing took 13.468296614009887 seconds)\n",
      "2024-11-01 21:12:51 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2024-11-01 21:12:51 | INFO | train | epoch 009 | loss 6.869 | nll_loss 5.835 | ppl 57.1 | wps 13518.1 | ups 3.73 | wpb 3626.3 | bsz 198.4 | num_updates 9093 | lr 0.000331624 | gnorm 6.639 | loss_scale 2 | train_wall 247 | gb_free 20.3 | wall 2435\n",
      "2024-11-01 21:12:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 21:12:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1011\n",
      "epoch 010:   0%|                                       | 0/1011 [00:00<?, ?it/s]2024-11-01 21:12:51 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2024-11-01 21:12:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 1010/1011 [04:05<00:00,  4.07it/s, loss=6.806, nll_loss=5.7642024-11-01 21:16:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 21:16:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   6%|▍      | 2/34 [00:00<00:01, 19.43it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  12%|▊      | 4/34 [00:00<00:01, 19.37it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  18%|█▏     | 6/34 [00:00<00:01, 18.57it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  24%|█▋     | 8/34 [00:00<00:01, 18.20it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  29%|█▊    | 10/34 [00:00<00:01, 17.62it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  35%|██    | 12/34 [00:00<00:01, 18.01it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  41%|██▍   | 14/34 [00:00<00:01, 17.56it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  47%|██▊   | 16/34 [00:00<00:01, 17.53it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  53%|███▏  | 18/34 [00:00<00:00, 18.00it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  59%|███▌  | 20/34 [00:01<00:00, 17.84it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  65%|███▉  | 22/34 [00:01<00:00, 17.40it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  71%|████▏ | 24/34 [00:01<00:00, 16.92it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  76%|████▌ | 26/34 [00:01<00:00, 16.91it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  82%|████▉ | 28/34 [00:01<00:00, 16.57it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  88%|█████▎| 30/34 [00:01<00:00, 16.77it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  94%|█████▋| 32/34 [00:01<00:00, 16.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 21:16:59 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.79 | nll_loss 6.758 | ppl 108.2 | wps 39005.4 | wpb 2192.2 | bsz 122.8 | num_updates 10104 | best_loss 6.284\n",
      "2024-11-01 21:16:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 10104 updates\n",
      "2024-11-01 21:16:59 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint10.pt\n",
      "2024-11-01 21:17:04 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/checkpoints/checkpoint10.pt\n",
      "2024-11-01 21:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 10104 updates, score 7.79) (writing took 13.4549848199822 seconds)\n",
      "2024-11-01 21:17:13 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2024-11-01 21:17:13 | INFO | train | epoch 010 | loss 6.851 | nll_loss 5.815 | ppl 56.28 | wps 13966 | ups 3.85 | wpb 3626 | bsz 198.4 | num_updates 10104 | lr 0.000314596 | gnorm 7.633 | loss_scale 2 | train_wall 243 | gb_free 20.4 | wall 2697\n",
      "2024-11-01 21:17:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 21:17:13 | INFO | fairseq_cli.train | done training in 2695.8 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make sure that (0.9, 0.98) in \"\", error might occurs when use ''.\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "    binarized_data/iwslt14.de-en \\\n",
    "    --arch mbart_large --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas \"(0.9, 0.98)\" --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --max-epoch 10 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate translations with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 21:19:02.870125: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 21:19:06.611487: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 21:19:06.611537: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 21:19:06.611574: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 21:19:07.163666: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 21:19:28.085310: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'binarized_data/iwslt14.de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "INFO:fairseq.tasks.translation:[de] dictionary: 10128 types\n",
      "INFO:fairseq.tasks.translation:[en] dictionary: 10128 types\n",
      "INFO:fairseq_cli.generate:loading model(s) from checkpoints/checkpoint_best.pt\n",
      "INFO:fairseq.data.data_utils:loaded 4,179 examples from: binarized_data/iwslt14.de-en/test.de-en.de\n",
      "INFO:fairseq.data.data_utils:loaded 4,179 examples from: binarized_data/iwslt14.de-en/test.de-en.en\n",
      "INFO:fairseq.tasks.translation:binarized_data/iwslt14.de-en test de-en 4179 examples\n",
      "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True\n",
      "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
      "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
      "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n",
      "INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2\n",
      "INFO:fairseq_cli.generate:Translated 4,176 sentences (57,429 tokens) in 41.5s (100.52 sentences/s, 1382.32 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓ Even delete this line, it works...xD\n",
    "# TEST_INPUT=\"dataset/spm.tst.de-en.de\"\n",
    "\n",
    "!fairseq-generate binarized_data/iwslt14.de-en \\\n",
    "      --task translation \\\n",
    "      --source-lang de \\\n",
    "      --target-lang en \\\n",
    "      --path checkpoints/checkpoint_best.pt \\\n",
    "      --batch-size 256 \\\n",
    "      --beam 4 \\\n",
    "      --remove-bpe=sentencepiece > \"en-de.decode.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the hypotheses and references from the decoding log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's water.\n",
      "I can show you.\n",
      "Now the good news.\n",
      "Oh.\n",
      "It's not about technology.\n",
      "It's not about technology.\n",
      "Well we had this.\n",
      "We've done this.\n",
      "Here's another group.\n",
      "You look at this process.\n",
      "\n",
      "The water is completely black.\n",
      "Where are the different countries?\n",
      "Now, the good news.\n",
      "Evan. Okay.\n",
      "It's not hardware.\n",
      "It's not software.\n",
      "Okay, we got that.\n",
      "And we reported this.\n",
      "Here's another group.\n",
      "Look at this complexity here.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep ^H \"en-de.decode.log\" | sed 's/^H-//g' | cut -f 3 | sed 's/ ##//g' > ./hyp.txt\n",
    "grep ^T \"en-de.decode.log\" | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > ./ref.txt\n",
    "head ./hyp.txt\n",
    "echo \"\"\n",
    "head ./ref.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use BLEU as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv\n",
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 7.9,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
      " \"verbose_score\": \"44.5/14.7/6.1/2.6 (BP = 0.781 ratio = 0.801 hyp_len = 44594 ref_len = 55644)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.4.3\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!echo $PWD\n",
    "!bash -c \"cat hyp.txt | sacrebleu ref.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in ./myEnv/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in ./myEnv/lib/python3.9/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in ./myEnv/lib/python3.9/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from rouge_score) (1.26.0)\n",
      "Requirement already satisfied: six>=1.14.0 in ./myEnv/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in ./myEnv/lib/python3.9/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./myEnv/lib/python3.9/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./myEnv/lib64/python3.9/site-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./myEnv/lib/python3.9/site-packages (from nltk->rouge_score) (4.66.6)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "ROUGE scores:\n",
      "rouge1: 0.3851\n",
      "rouge2: 0.1604\n",
      "rougeL: 0.3541\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!bash -c \"cat hyp.txt | python compute_rouge.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
