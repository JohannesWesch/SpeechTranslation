{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba54f9-ddf3-4ed3-8c05-17c8f78a845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"/pfs/data5/home/kit/stud/u____/.conda/envs/salmonn/lib/python3.9/site-packages:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PATH\"] = \"/pfs/data5/home/kit/stud/u____/.conda/envs/salmonn/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe2853-3330-4a29-a3f6-e0b64b951fec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2a566-cde0-4767-a581-d036bb4ac02a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Install SALMONN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d604-c912-40a1-9a8c-fd8bccae25b2",
   "metadata": {},
   "source": [
    "Before we begin, load miniconda from the left sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc36aa3-4a45-4896-bfcb-44b5e3d37242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/bytedance/SALMONN.git\n",
    "\n",
    "# !conda create -n salmonn python=3.9.17\n",
    "# !conda activate salmonn\n",
    "\n",
    "# !conda install pytorch torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "# !pip install peft==0.3.0\n",
    "# !pip install soundfile\n",
    "# !pip install librosa\n",
    "# !pip install transformers==4.28.0\n",
    "# !pip install sentencepiece==0.1.97\n",
    "# !pip install accelerate==0.20.3\n",
    "# !pip install bitsandbytes==0.35.0\n",
    "# !pip install gradio==3.23.0\n",
    "\n",
    "# !pip install omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7373d5-f647-4e45-9adf-0c0e96bfc85b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 Download models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83041063-1ba6-4a41-8af7-863591c2bf62",
   "metadata": {},
   "source": [
    "They are sorted here: /pfs/data5/home/kit/stud/u____/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7a896-6d9a-473b-bf46-9f22d54d2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download openai/whisper-large-v2\n",
    "# !huggingface-cli download lmsys/vicuna-7b-v1.5\n",
    "# !huggingface-cli download tsinghua-ee/SALMONN-7B\n",
    "\n",
    "# The BEATs model can be downloaded from: https://www.google.com/url?q=https%3A%2F%2F1drv.ms%2Fu%2Fs%21AqeByhGUtINrgcpj8ujXH1YUtxooEg%3Fe%3DE9Ncea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5732c1-39b8-419e-bc86-19685725fb01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf79373-8888-41c7-9937-233ca4d969f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee477e1a-591b-4c7f-86b5-3a1191e95ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==2.7.1\n",
    "# !pip install huggingface_hub\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(token=\"your token\")\n",
    "# dataset = load_dataset(\"mozilla-foundation/common_voice_4_0\", \"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d118d29-11a1-4b95-a70d-7149c6650cfe",
   "metadata": {},
   "source": [
    "Download CoVoST2 translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c65c6b-4521-4cb0-a5d1-c0aa6736db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CoVoST2 translations from:\n",
    "# https://dl.fbaipublicfiles.com/covost/covost_v2.de_en.tsv.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ebaf6-6f23-44d9-8d8f-c96604a83469",
   "metadata": {},
   "source": [
    "Get data splits. Download this script from [CoVoST2](https://github.com/facebookresearch/covost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da033979-8f1d-4b6d-b79f-b8650d6efe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 get_covost_splits.py \\\n",
    "#   --version 2 --src-lang de --tgt-lang en \\\n",
    "#   --root tsv_for_S3 \\\n",
    "#   --cv-tsv /pfs/data5/home/kit/stud/u____/.cache/huggingface/datasets/downloads/extracted/____/validated.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83967a-c52f-452a-9178-99c8441da290",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f3723-31a6-476b-9ca0-270ff45290e4",
   "metadata": {},
   "source": [
    "We should format our data such that each sample consist of:\n",
    "- Path to the audio  \n",
    "- Target text (translation)  \n",
    "- Task name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46c7f4-981e-4177-8816-3af77d62b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def convert_tsv_to_json(tsv_path, json_path):\n",
    "    annotations = []\n",
    "\n",
    "    with open(tsv_path, 'r', encoding='utf-8') as tsv_file:\n",
    "        reader = csv.DictReader(tsv_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            annotation = {\n",
    "                \"path\": \"/pfs/data5/home/kit/stud/u____/.cache/huggingface/datasets/downloads/extracted/____/clips/\" + row[\"path\"],\n",
    "                \"text\": row[\"translation\"],\n",
    "                \"task\": \"translation_de2en\"\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump({\"annotation\": annotations}, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Conversion\n",
    "file_map = {\n",
    "    \"tsv_for_S3/covost_v2.de_en.train.tsv\": \"tsv_for_S3/train.json\",\n",
    "    \"tsv_for_S3/covost_v2.de_en.dev.tsv\": \"tsv_for_S3/dev.json\",\n",
    "    \"tsv_for_S3/covost_v2.de_en.test.tsv\": \"tsv_for_S3/test.json\"\n",
    "}\n",
    "\n",
    "for tsv, json_file in file_map.items():\n",
    "    convert_tsv_to_json(tsv, json_file)\n",
    "    print(f\"converted to {json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed62697-1694-43f3-8bbf-ae8741b1ab87",
   "metadata": {},
   "source": [
    "Convert mp3 to wav if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598116f-57c1-474c-a628-7a299d4f4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "json_files = [\"train.json\", \"dev.json\", \"test.json\"]\n",
    "\n",
    "for json_file in json_files:\n",
    "    print(f\"正在处理 {json_file}...\")\n",
    "\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    annotations = data.get(\"annotation\", [])\n",
    "    \n",
    "    for item in tqdm(annotations, desc=f\"处理 {json_file}\"):\n",
    "        mp3_path = item[\"path\"]\n",
    "        wav_path = os.path.splitext(mp3_path)[0] + \".wav\"\n",
    "\n",
    "        # 检查 MP3 文件是否存在\n",
    "        if not os.path.exists(mp3_path):\n",
    "            print(f\"MP3 文件不存在，跳过: {mp3_path}\")\n",
    "            continue\n",
    "\n",
    "        # 检查目标 WAV 文件是否已存在\n",
    "        if os.path.exists(wav_path):\n",
    "            print(f\"WAV 文件已存在，跳过: {wav_path}\")\n",
    "            continue\n",
    "\n",
    "        wav_dir = os.path.dirname(wav_path)\n",
    "        if not os.path.exists(wav_dir):\n",
    "            os.makedirs(wav_dir)\n",
    "\n",
    "        # 加载 MP3 文件并转换为 WAV\n",
    "        try:\n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            audio.export(wav_path, format=\"wav\", parameters=[\"-ar\", \"16000\"])\n",
    "            print(f\"已生成 WAV 文件: {wav_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"转换失败: {mp3_path}, 错误: {e}\")\n",
    "    \n",
    "    print(f\"{json_file} 处理完成！\")\n",
    "\n",
    "print(\"所有 JSON 文件已处理完成！\")\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "for json_file in json_files:\n",
    "    print(f\"正在处理 {json_file}...\")\n",
    "\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if \"annotation\" in data and isinstance(data[\"annotation\"], list):\n",
    "        for item in tqdm(data[\"annotation\"], desc=f\"处理 {json_file}\"):\n",
    "            if \"path\" in item and item[\"path\"].endswith(\".mp3\"):\n",
    "                item[\"path\"] = os.path.splitext(item[\"path\"])[0] + \".wav\"\n",
    "\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"{json_file} 修改完成！\")\n",
    "\n",
    "print(\"所有 JSON 文件的路径已更新为 .wav！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb15cc7-103d-4d72-b626-5605cd76134d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Translation with raw SALMONN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059f963-d32d-4b9e-953a-1c9f946fe59c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1 Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f876671-3244-40a1-a36d-6ecf96dc8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify cli_inderence.py & decode_config.yaml\n",
    "# resample wav file in utils.py if sr!=16k\n",
    "# Tesla V100 doesn't support Int8: pip install bitsandbytes==0.37.2\n",
    "!python3 SALMONN/cli_inference.py --cfg-path SALMONN/configs/decode_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d3e51-0bf4-4525-90db-65725ce4cbe3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1118b0-63c7-4227-99c1-d68beaea58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82d8bf-1861-4780-abfd-9a3f3f499562",
   "metadata": {},
   "source": [
    "# 4. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5aa277-1882-4632-83ab-872746697ab9",
   "metadata": {},
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3ffbe-3c2c-44ca-903c-fdb71756a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify config.yaml and runner.py so that it can continue training from the position of last epoch.\n",
    "# modify 'save_checkpoint' function to save all parameters.\n",
    "# set num_workers = 0\n",
    "!python3 SALMONN/train.py --cfg-path SALMONN/configs/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbc6b1-23ff-4113-9642-e395047fabe3",
   "metadata": {},
   "source": [
    "## 4.2 Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c441db-d266-4870-9276-afc899b733e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fine-tuned model\n",
    "!python3 SALMONN/cli_inference.py --cfg-path SALMONN/configs/decode_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cb3ce-adce-41f4-9f37-fd2c354cdbd7",
   "metadata": {},
   "source": [
    "## 4.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fedd5b2a-526e-4ecf-a3a9-fdd4535204ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 29.46351347844518\n"
     ]
    }
   ],
   "source": [
    "# BLEU\n",
    "import sacrebleu\n",
    "\n",
    "# Load the hypothesis and reference files\n",
    "with open('tsv_for_S3/hyp_output_after5.txt', 'r') as f:\n",
    "    hypothesis = f.readlines()\n",
    "\n",
    "with open('tsv_for_S3/ref_output_after5.txt', 'r') as f:\n",
    "    reference = f.readlines()\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(hypothesis, [reference])\n",
    "\n",
    "# Print the BLEU score\n",
    "print(f\"BLEU score: {bleu.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc963b0c-92cc-4305-9d36-823b01cc74aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Salmonn",
   "language": "python",
   "name": "salmonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
