{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TYL7jnSoyvI"
      },
      "source": [
        "# 1. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GyQ4QmuoyvJ"
      },
      "source": [
        "## 1.1 Install pip and torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CXp1EwvmoyvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ea97bc-02c4-47ce-a8ed-228623ead68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip==24.0 in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "Version: 2.5.0+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip install pip==24.0\n",
        "!pip show torch | grep Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xET5s0CoyvK"
      },
      "source": [
        "## 1.2 Install fairseq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjHYFYXvoyvK"
      },
      "source": [
        "First install fairseq, trust me, the original installation method sucks..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nu6XrdMqoyvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "81188187-2767-4c3d-9bfb-3ea5c53a8e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35385, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 35385 (delta 10), reused 40 (delta 9), pack-reused 35337 (from 1)\u001b[K\n",
            "Receiving objects: 100% (35385/35385), 25.47 MiB | 20.59 MiB/s, done.\n",
            "Resolving deltas: 100% (25539/25539), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.26.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2024.9.11)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.3)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.5)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.5.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.1)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.10.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (3.0.2)\n",
            "Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9554 sha256=ad6e6f5d3e36aac250b87822ddacec0ddb6085f10a05c9d6ff5921ca17baa130\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vifrbf11/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=49bd24e48a9dac405a2b18d8b17b3eb8820f775c0b5884e20dd448c96f7353a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.3.0\n",
            "    Uninstalling omegaconf-2.3.0:\n",
            "      Successfully uninstalled omegaconf-2.3.0\n",
            "  Attempting uninstall: hydra-core\n",
            "    Found existing installation: hydra-core 1.3.2\n",
            "    Uninstalling hydra-core-1.3.2:\n",
            "      Successfully uninstalled hydra-core-1.3.2\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.12.3\n",
            "    Uninstalling fairseq-0.12.3:\n",
            "      Successfully uninstalled fairseq-0.12.3\n",
            "Successfully installed antlr4-python3-runtime-4.8 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/fairseq.git\n",
        "%cd fairseq\n",
        "!pip install --editable ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Ms408FoyvL"
      },
      "source": [
        "Then we need to add a new environment variable so that we can use the fairseq command in the terminal.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Uzz0OANoyvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1dd009-ccac-4feb-a183-43ac09299060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/env/python\n",
            "/env/python:/content/fairseq/\n"
          ]
        }
      ],
      "source": [
        "!echo $PYTHONPATH\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kfjRHR8oyvL"
      },
      "source": [
        "## 1.3 Install other packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uH9i8_HBoyvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "75ce1ea0-727a-41c4-f7d1-b3eecdd3f06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/897.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m890.9/897.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses\n",
        "!pip install sentencepiece\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKdzNHIboyvM"
      },
      "source": [
        "## 1.4 Activate GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6TedPDXGoyvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312ea54c-dd9e-4339-8b6e-2ece1ba03e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device:  Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device(); print('Current device: ', torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    device = 'cpu'; print('Current device: CPU.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enm4g2LYoyvM"
      },
      "source": [
        "# 2. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA5SMaUVoyvM"
      },
      "source": [
        "## 2.1 Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dlkp_du2oyvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "0bcd63b7-1875-4162-e8c2-0423bfd3c22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fairseq/examples/translation\n",
            "--2024-10-30 15:38:28--  https://bwsyncandshare.kit.edu/s/Xx3D56SJmG8PwXj/download\n",
            "Resolving bwsyncandshare.kit.edu (bwsyncandshare.kit.edu)... 141.3.135.129, 2a00:1398:b::8d03:8781\n",
            "Connecting to bwsyncandshare.kit.edu (bwsyncandshare.kit.edu)|141.3.135.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘sample_data.zip’\n",
            "\n",
            "sample_data.zip         [              <=>   ]  33.03M  7.79MB/s    in 4.2s    \n",
            "\n",
            "2024-10-30 15:38:34 (7.79 MB/s) - ‘sample_data.zip’ saved [34633335]\n",
            "\n",
            "Archive:  sample_data.zip\n",
            "   creating: sample_data/\n",
            " extracting: sample_data/dev.wikimedia.de-en.de  \n",
            " extracting: sample_data/dev.wikimedia.de-en.en  \n",
            " extracting: sample_data/train.wikimedia.de-en.de  \n",
            " extracting: sample_data/train.wikimedia.de-en.en  \n",
            " extracting: sample_data/tst.wikimedia.de-en.de  \n",
            " extracting: sample_data/tst.wikimedia.de-en.en  \n"
          ]
        }
      ],
      "source": [
        "%cd /content/fairseq/examples/translation\n",
        "\n",
        "!wget -O sample_data.zip https://bwsyncandshare.kit.edu/s/Xx3D56SJmG8PwXj/download\n",
        "# If wget command not found, download the wget.exe from this website and move it to C:\\Windows\\System32: https://eternallybored.org/misc/wget/\n",
        "\n",
        "!unzip -o sample_data.zip\n",
        "# If unzip command not found, download \"Complete package, except sources\" and copy unzip.exe to C:\\Windows: https://gnuwin32.sourceforge.net/packages/unzip.htm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2KtLbYvoyvM"
      },
      "source": [
        "## 2.2 Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in downloaded `sample_data`\n",
        "!ls -ltr sample_data\n",
        "\n",
        "!echo -e \"\\nFirst lines of German:\\n\"\n",
        "!head sample_data/train.wikimedia.de-en.de\n",
        "!echo -e \"\\nFirst lines of English:\\n\"\n",
        "!head sample_data/train.wikimedia.de-en.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ydo3DNZm3ky_",
        "outputId": "9bed873c-4c44-4eca-d0d1-7f5f5269b452"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 33832\n",
            "-rw-r--r-- 1 root root   335913 Oct 30 15:26 tst.wikimedia.de-en.en\n",
            "-rw-r--r-- 1 root root   347217 Oct 30 15:26 tst.wikimedia.de-en.de\n",
            "-rw-r--r-- 1 root root 16015279 Oct 30 15:26 train.wikimedia.de-en.en\n",
            "-rw-r--r-- 1 root root 17268516 Oct 30 15:26 train.wikimedia.de-en.de\n",
            "-rw-r--r-- 1 root root   320077 Oct 30 15:26 dev.wikimedia.de-en.en\n",
            "-rw-r--r-- 1 root root   345243 Oct 30 15:26 dev.wikimedia.de-en.de\n",
            "\n",
            "First lines of German:\n",
            "\n",
            "Er war Aufsichtsratsvorsitzender der Bahngesellschaften Chesapeake and Ohio Railway und New York Central Railroad. \n",
            "Youngs Großvater war ein Rancher im [[Texas Panhandle]]. \n",
            "Er wurde als drittes von vier Kindern geboren. \n",
            "Seine Mutter starb als er zehn Jahre alt war. \n",
            "Er besuchte die von der Baptistischen Kirche unterhaltene Canadian Academy. \n",
            "Als Teenager absolvierte er die [[Culver Military Academy]] in [[Culver (Indiana)]]. \n",
            "Diese schloss er 1914 als Klassenbester ab. Anschließend begann er ein Studium an der [[University of Virginia]] in [[Charlottesville]]. \n",
            "Dieses beendete er jedoch am Ende des zweiten Studienjahres. \n",
            "Robert R. Young begann eine Tätigkeit Pulverschneider im Sprengstoffwerk von [[E. I. DuPont]] in Carneys Point (New Jersey). Am 27. April 1916 heiratete er [[Anita Ten Eyck O'Keeffe]]. Das Ehepaar hatte eine Tochter. \n",
            "Bei DuPont arbeitete er später in der Rechnungsabteilung und erwarb dort Wissen zum Finanzwesen und zur Werbung. 1920 verließ er das Unternehmen und begann mit [[Sicherheit]]en zu spekulieren. \n",
            "\n",
            "First lines of English:\n",
            "\n",
            "He was a brother-in-law of the famous western painter Georgia O'Keeffe. \n",
            "Young's grandfather was a pioneer rancher in the Texas Panhandle. \n",
            "Young was the third of four children (John Stinson Young, b. \n",
            "1890; Kenneth Moody Young, b. \n",
            "1893; Robert Ralph Young, b. 1897; Florence Edith Young Exum, b. \n",
            "1904) born to David John Young and the former Mary Arabella Moody in Canadian, the seat of Hemphill County in the eastern Panhandle. \n",
            "He was born in a house built by Temple Lea Houston, youngest son of Sam Houston, first president of the Republic of Texas and a later governor of Texas.[1] Mary Moody Young's father built the Moody Hotel in Canadian, which still houses a few businesses. \n",
            "Mary died when Robert was only ten. \n",
            "David Young, a strict man and the first banker in Canadian, did not quite know how to control his precocious son, whom neighborhood boys nicknamed \"Pumpkin\" because of his auburn hair. \n",
            "Young took a job as a powder-cutter at the E. I. DuPont gunpowder plant at Carneys Point, New Jersey. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEk4ElBgoyvM"
      },
      "source": [
        "Segment the text into subwords using BPE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NlAmkcoFoyvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800a2293-0c8f-420a-8e64-717f92ee96a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished training sentencepiece model.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# After execution, you can find two bpe files in the directory.\n",
        "spm.SentencePieceTrainer.train(input=\"sample_data/train.wikimedia.de-en.de,sample_data/train.wikimedia.de-en.en\",\n",
        "                               model_prefix=\"bpe\",\n",
        "                               vocab_size=10000)\n",
        "\n",
        "print('Finished training sentencepiece model.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcW0K-L9oyvN"
      },
      "source": [
        "Then we use the trained segmentation model to preprocess the sentences from train/dev/test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9uYl22CaoyvN"
      },
      "outputs": [],
      "source": [
        "# Load the trained sentencepiece model\n",
        "spm_model = spm.SentencePieceProcessor(model_file=\"bpe.model\")\n",
        "\n",
        "# Important: encoding=\"utf-8\"\n",
        "for partition in [\"train\", \"dev\", \"tst\"]:\n",
        "    for lang in [\"de\", \"en\"]:\n",
        "        f_out = open(f\"sample_data/spm.{partition}.wikimedia.de-en.{lang}\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "        with open(f\"sample_data/{partition}.wikimedia.de-en.{lang}\", \"r\", encoding=\"utf-8\") as f_in:\n",
        "            for line_idx, line in enumerate(f_in.readlines()):\n",
        "                # Segmented into subwords\n",
        "                line_segmented = spm_model.encode(line.strip(), out_type=str)\n",
        "                # Join the subwords into a string\n",
        "                line_segmented = \" \".join(line_segmented)\n",
        "                f_out.write(line_segmented + \"\\n\")\n",
        "\n",
        "        f_out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho-VSblKoyvN"
      },
      "source": [
        "Now, we will binarize the data for training with fairseq.  \n",
        "Feel free to check the [documentation](https://fairseq.readthedocs.io/en/latest/command_line_tools.html) of fairseq commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yjdIBeNToyvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03be3365-1172-40b6-abd9-31385c7403a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fairseq/examples/translation/sample_data\n",
            "2024-10-30 15:46:47.758228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-30 15:46:48.009347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-30 15:46:48.083353: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-30 15:46:48.509267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-30 15:46:50.122160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/fairseq/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='/content/fairseq/examples/translation/sample_data/spm.train.wikimedia.de-en', validpref='/content/fairseq/examples/translation/sample_data/spm.dev.wikimedia.de-en', testpref='/content/fairseq/examples/translation/sample_data/spm.tst.wikimedia.de-en', align_suffix=None, destdir='data-bin/iwslt14.de-en', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)\n",
            "INFO:fairseq_cli.preprocess:[de] Dictionary: 11512 types\n",
            "INFO:fairseq_cli.preprocess:[de] /content/fairseq/examples/translation/sample_data/spm.train.wikimedia.de-en.de: 126803 sents, 4507807 tokens, 0.0% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[de] Dictionary: 11512 types\n",
            "INFO:fairseq_cli.preprocess:[de] /content/fairseq/examples/translation/sample_data/spm.dev.wikimedia.de-en.de: 2642 sents, 90990 tokens, 0.033% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[de] Dictionary: 11512 types\n",
            "INFO:fairseq_cli.preprocess:[de] /content/fairseq/examples/translation/sample_data/spm.tst.wikimedia.de-en.de: 2642 sents, 90105 tokens, 0.0111% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[en] Dictionary: 10600 types\n",
            "INFO:fairseq_cli.preprocess:[en] /content/fairseq/examples/translation/sample_data/spm.train.wikimedia.de-en.en: 126803 sents, 4293410 tokens, 0.0% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[en] Dictionary: 10600 types\n",
            "INFO:fairseq_cli.preprocess:[en] /content/fairseq/examples/translation/sample_data/spm.dev.wikimedia.de-en.en: 2642 sents, 86566 tokens, 0.187% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[en] Dictionary: 10600 types\n",
            "INFO:fairseq_cli.preprocess:[en] /content/fairseq/examples/translation/sample_data/spm.tst.wikimedia.de-en.en: 2642 sents, 88462 tokens, 0.131% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:Wrote preprocessed data to data-bin/iwslt14.de-en\n"
          ]
        }
      ],
      "source": [
        "# Preprocess/binarize the data\n",
        "TEXT=\"/content/fairseq/examples/translation/sample_data\"\n",
        "!echo $TEXT\n",
        "# Binarize the data for training\n",
        "!fairseq-preprocess \\\n",
        "    --source-lang de --target-lang en \\\n",
        "    --trainpref $TEXT/spm.train.wikimedia.de-en \\\n",
        "    --validpref $TEXT/spm.dev.wikimedia.de-en \\\n",
        "    --testpref $TEXT/spm.tst.wikimedia.de-en \\\n",
        "    --destdir data-bin/iwslt14.de-en \\\n",
        "    --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcibk4KaoyvN"
      },
      "source": [
        "The data preprocessing is completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo0HfrH-oyvN"
      },
      "source": [
        "# 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "68RdHhFfoyvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b9c474-0886-4d25-9e95-f2907acf6129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-30 15:52:00.802100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-30 15:52:00.822047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-30 15:52:00.827996: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-30 15:52:00.843356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-30 15:52:01.904808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-10-30 15:52:03 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-10-30 15:52:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-10-30 15:52:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/content/fairseq/examples/translation/data-bin/iwslt14.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, max_source_positions=4096, max_target_positions=4096, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/content/fairseq/examples/translation/data-bin/iwslt14.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 4096, 'max_target_positions': 4096, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2024-10-30 15:52:06 | INFO | fairseq.tasks.translation | [de] dictionary: 11512 types\n",
            "2024-10-30 15:52:06 | INFO | fairseq.tasks.translation | [en] dictionary: 10600 types\n",
            "2024-10-30 15:52:07 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(11512, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(10600, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=10600, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-10-30 15:52:07 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-10-30 15:52:07 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-10-30 15:52:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-10-30 15:52:07 | INFO | fairseq_cli.train | num. shared model params: 55,459,840 (num. trained: 55,459,840)\n",
            "2024-10-30 15:52:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-10-30 15:52:07 | INFO | fairseq.data.data_utils | loaded 2,642 examples from: /content/fairseq/examples/translation/data-bin/iwslt14.de-en/valid.de-en.de\n",
            "2024-10-30 15:52:07 | INFO | fairseq.data.data_utils | loaded 2,642 examples from: /content/fairseq/examples/translation/data-bin/iwslt14.de-en/valid.de-en.en\n",
            "2024-10-30 15:52:07 | INFO | fairseq.tasks.translation | /content/fairseq/examples/translation/data-bin/iwslt14.de-en valid de-en 2642 examples\n",
            "2024-10-30 15:52:08 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-10-30 15:52:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-10-30 15:52:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-10-30 15:52:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-10-30 15:52:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-10-30 15:52:08 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2024-10-30 15:52:08 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2024-10-30 15:52:08 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2024-10-30 15:52:08 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-10-30 15:52:08 | INFO | fairseq.data.data_utils | loaded 126,803 examples from: /content/fairseq/examples/translation/data-bin/iwslt14.de-en/train.de-en.de\n",
            "2024-10-30 15:52:08 | INFO | fairseq.data.data_utils | loaded 126,803 examples from: /content/fairseq/examples/translation/data-bin/iwslt14.de-en/train.de-en.en\n",
            "2024-10-30 15:52:08 | INFO | fairseq.tasks.translation | /content/fairseq/examples/translation/data-bin/iwslt14.de-en train de-en 126803 examples\n",
            "2024-10-30 15:52:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 15:52:08 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2024-10-30 15:52:08 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2024-10-30 15:52:08 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2024-10-30 15:52:09 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2024-10-30 15:52:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 15:52:09 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2024-10-30 15:52:09 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2024-10-30 15:52:09 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2024-10-30 15:52:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 001:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 15:52:09 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-10-30 15:52:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 001:   3% 41/1433 [00:07<02:40,  8.68it/s]2024-10-30 15:52:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:  14% 206/1433 [00:27<02:18,  8.83it/s, loss=11.615, nll_loss=11.328, ppl=2571.13, wps=23611.4, ups=8.06, wpb=2929.3, bsz=71.8, num_updates=200, lr=2.5e-05, gnorm=1.5, loss_scale=64, train_wall=12, gb_free=12.5, wall=29]2024-10-30 15:52:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:  18% 255/1433 [00:33<02:35,  7.56it/s, loss=11.615, nll_loss=11.328, ppl=2571.13, wps=23611.4, ups=8.06, wpb=2929.3, bsz=71.8, num_updates=200, lr=2.5e-05, gnorm=1.5, loss_scale=64, train_wall=12, gb_free=12.5, wall=29]2024-10-30 15:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  33% 478/1433 [01:02<02:11,  7.24it/s, loss=10.594, nll_loss=10.129, ppl=1120.17, wps=22575.2, ups=7.81, wpb=2890.3, bsz=97, num_updates=400, lr=5e-05, gnorm=1.595, loss_scale=16, train_wall=12, gb_free=12.5, wall=54]2024-10-30 15:53:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 001: 100% 1432/1433 [03:09<00:00,  7.68it/s, loss=9.561, nll_loss=8.907, ppl=480.01, wps=22597.6, ups=7.54, wpb=2998.1, bsz=73.8, num_updates=1400, lr=0.000175, gnorm=1.353, loss_scale=8, train_wall=13, gb_free=13, wall=188]2024-10-30 15:55:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 15:55:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/46 [00:00<00:04,  9.43it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   7% 3/46 [00:00<00:02, 14.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 5/46 [00:00<00:03, 13.38it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 7/46 [00:00<00:03, 12.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 9/46 [00:00<00:02, 14.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 17.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 19.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  39% 18/46 [00:01<00:01, 20.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 20.18it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 21.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 27/46 [00:01<00:00, 21.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  65% 30/46 [00:01<00:00, 21.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  74% 34/46 [00:01<00:00, 24.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 38/46 [00:01<00:00, 27.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 42/46 [00:01<00:00, 28.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 46/46 [00:02<00:00, 30.24it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 15:55:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.318 | nll_loss 8.604 | ppl 389.19 | wps 43554.4 | wpb 1880.3 | bsz 57.4 | num_updates 1429\n",
            "2024-10-30 15:55:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1429 updates\n",
            "2024-10-30 15:55:22 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint1.pt\n",
            "2024-10-30 15:55:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint1.pt\n",
            "2024-10-30 15:55:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 1429 updates, score 9.318) (writing took 11.130455565000148 seconds)\n",
            "2024-10-30 15:55:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-10-30 15:55:33 | INFO | train | epoch 001 | loss 10.381 | nll_loss 9.866 | ppl 933.17 | wps 21274 | ups 7.1 | wpb 2995.1 | bsz 86.5 | num_updates 1429 | lr 0.000178625 | gnorm 1.572 | loss_scale 8 | train_wall 181 | gb_free 12.7 | wall 205\n",
            "2024-10-30 15:55:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 15:55:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 002:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 15:55:33 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-10-30 15:55:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1432/1433 [03:09<00:00,  7.58it/s, loss=8.708, nll_loss=7.917, ppl=241.7, wps=22136.5, ups=7.3, wpb=3032.4, bsz=106.4, num_updates=2800, lr=0.00035, gnorm=1.21, loss_scale=8, train_wall=13, gb_free=12.4, wall=387]2024-10-30 15:58:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 15:58:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/46 [00:00<00:04,  9.28it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   7% 3/46 [00:00<00:03, 12.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 5/46 [00:00<00:03, 12.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 7/46 [00:00<00:02, 14.25it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 10/46 [00:00<00:02, 16.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 19.85it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 21.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 24.89it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 23/46 [00:01<00:00, 26.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 27/46 [00:01<00:00, 27.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  65% 30/46 [00:01<00:00, 27.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  74% 34/46 [00:01<00:00, 29.13it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 38/46 [00:01<00:00, 31.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 42/46 [00:01<00:00, 31.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 46/46 [00:01<00:00, 31.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 15:58:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.52 | nll_loss 7.664 | ppl 202.81 | wps 49557.4 | wpb 1880.3 | bsz 57.4 | num_updates 2862 | best_loss 8.52\n",
            "2024-10-30 15:58:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2862 updates\n",
            "2024-10-30 15:58:44 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint2.pt\n",
            "2024-10-30 15:58:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint2.pt\n",
            "2024-10-30 15:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 2862 updates, score 8.52) (writing took 11.461149334999845 seconds)\n",
            "2024-10-30 15:58:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-10-30 15:58:56 | INFO | train | epoch 002 | loss 9.009 | nll_loss 8.264 | ppl 307.51 | wps 21156.4 | ups 7.06 | wpb 2995.2 | bsz 88.5 | num_updates 2862 | lr 0.00035775 | gnorm 1.252 | loss_scale 8 | train_wall 180 | gb_free 12.4 | wall 408\n",
            "2024-10-30 15:58:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 15:58:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 003:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 15:58:56 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-10-30 15:58:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1432/1433 [03:04<00:00,  7.65it/s, loss=8.213, nll_loss=7.349, ppl=162.99, wps=23167.1, ups=7.64, wpb=3030.5, bsz=104.8, num_updates=4200, lr=0.00048795, gnorm=1.08, loss_scale=8, train_wall=13, gb_free=12.5, wall=581]2024-10-30 16:02:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:02:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.24it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 26.84it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 28.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  43% 20/46 [00:00<00:00, 29.37it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 23/46 [00:00<00:00, 29.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 27/46 [00:00<00:00, 29.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  65% 30/46 [00:01<00:00, 29.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  74% 34/46 [00:01<00:00, 30.40it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 38/46 [00:01<00:00, 31.98it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 42/46 [00:01<00:00, 32.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 46/46 [00:01<00:00, 32.72it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:02:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.122 | nll_loss 7.17 | ppl 144 | wps 58044.1 | wpb 1880.3 | bsz 57.4 | num_updates 4295 | best_loss 8.122\n",
            "2024-10-30 16:02:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4295 updates\n",
            "2024-10-30 16:02:02 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint3.pt\n",
            "2024-10-30 16:02:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint3.pt\n",
            "2024-10-30 16:02:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 4295 updates, score 8.122) (writing took 17.797027697999965 seconds)\n",
            "2024-10-30 16:02:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-10-30 16:02:20 | INFO | train | epoch 003 | loss 8.408 | nll_loss 7.571 | ppl 190.19 | wps 20967 | ups 7 | wpb 2995.2 | bsz 88.5 | num_updates 4295 | lr 0.000482523 | gnorm 1.093 | loss_scale 8 | train_wall 177 | gb_free 12.5 | wall 613\n",
            "2024-10-30 16:02:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:02:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 004:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:02:21 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-10-30 16:02:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1432/1433 [03:05<00:00,  8.77it/s, loss=7.774, nll_loss=6.845, ppl=115, wps=23547.6, ups=7.74, wpb=3040.8, bsz=98.8, num_updates=5700, lr=0.000418854, gnorm=0.959, loss_scale=8, train_wall=12, gb_free=13, wall=795]2024-10-30 16:05:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:05:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 21.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 23.02it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 25.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 27.91it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 28.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  43% 20/46 [00:00<00:00, 29.11it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 23/46 [00:00<00:00, 29.22it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 27/46 [00:00<00:00, 29.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  65% 30/46 [00:01<00:00, 28.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  72% 33/46 [00:01<00:00, 27.62it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 37/46 [00:01<00:00, 28.54it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 41/46 [00:01<00:00, 29.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 44/46 [00:01<00:00, 28.92it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:05:28 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.755 | nll_loss 6.754 | ppl 107.94 | wps 54575.8 | wpb 1880.3 | bsz 57.4 | num_updates 5728 | best_loss 7.755\n",
            "2024-10-30 16:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5728 updates\n",
            "2024-10-30 16:05:28 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint4.pt\n",
            "2024-10-30 16:05:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint4.pt\n",
            "2024-10-30 16:05:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 5728 updates, score 7.755) (writing took 17.188436969000122 seconds)\n",
            "2024-10-30 16:05:45 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-10-30 16:05:45 | INFO | train | epoch 004 | loss 7.936 | nll_loss 7.031 | ppl 130.76 | wps 20919.3 | ups 6.98 | wpb 2995.2 | bsz 88.5 | num_updates 5728 | lr 0.000417829 | gnorm 0.991 | loss_scale 8 | train_wall 178 | gb_free 12.5 | wall 818\n",
            "2024-10-30 16:05:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:05:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 005:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:05:46 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-10-30 16:05:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1432/1433 [03:09<00:00,  7.64it/s, loss=7.534, nll_loss=6.569, ppl=94.97, wps=22914.8, ups=7.5, wpb=3056.9, bsz=87.5, num_updates=7100, lr=0.000375293, gnorm=0.932, loss_scale=8, train_wall=13, gb_free=12.8, wall=999]2024-10-30 16:08:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:08:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 5/46 [00:00<00:02, 19.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 23.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 26.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 27.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  43% 20/46 [00:00<00:00, 28.35it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 23/46 [00:00<00:00, 28.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  57% 26/46 [00:00<00:00, 28.80it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  63% 29/46 [00:01<00:00, 28.63it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  70% 32/46 [00:01<00:00, 28.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 36/46 [00:01<00:00, 30.72it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 40/46 [00:01<00:00, 30.82it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 44/46 [00:01<00:00, 30.88it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:08:57 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.666 | nll_loss 6.64 | ppl 99.76 | wps 55930.2 | wpb 1880.3 | bsz 57.4 | num_updates 7161 | best_loss 7.666\n",
            "2024-10-30 16:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7161 updates\n",
            "2024-10-30 16:08:57 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint5.pt\n",
            "2024-10-30 16:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint5.pt\n",
            "2024-10-30 16:09:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 7161 updates, score 7.666) (writing took 21.822592848999648 seconds)\n",
            "2024-10-30 16:09:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-10-30 16:09:19 | INFO | train | epoch 005 | loss 7.605 | nll_loss 6.65 | ppl 100.45 | wps 20139.9 | ups 6.72 | wpb 2995.2 | bsz 88.5 | num_updates 7161 | lr 0.000373691 | gnorm 0.956 | loss_scale 8 | train_wall 180 | gb_free 12.4 | wall 1031\n",
            "2024-10-30 16:09:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:09:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 006:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:09:19 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-10-30 16:09:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 1432/1433 [03:05<00:00,  8.22it/s, loss=7.307, nll_loss=6.309, ppl=79.29, wps=23206, ups=7.7, wpb=3014.9, bsz=100.8, num_updates=8500, lr=0.000342997, gnorm=0.975, loss_scale=8, train_wall=12, gb_free=12.7, wall=1205]2024-10-30 16:12:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:12:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 22.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 22.67it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 26.64it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 28.49it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  39% 18/46 [00:00<00:00, 29.22it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 22/46 [00:00<00:00, 29.65it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  57% 26/46 [00:00<00:00, 30.38it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 30/46 [00:01<00:00, 29.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 34/46 [00:01<00:00, 30.35it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 38/46 [00:01<00:00, 32.14it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 42/46 [00:01<00:00, 32.37it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 46/46 [00:01<00:00, 33.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:12:26 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.494 | nll_loss 6.441 | ppl 86.9 | wps 58907.5 | wpb 1880.3 | bsz 57.4 | num_updates 8594 | best_loss 7.494\n",
            "2024-10-30 16:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8594 updates\n",
            "2024-10-30 16:12:26 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint6.pt\n",
            "2024-10-30 16:12:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint6.pt\n",
            "2024-10-30 16:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 8594 updates, score 7.494) (writing took 24.05221593599981 seconds)\n",
            "2024-10-30 16:12:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-10-30 16:12:51 | INFO | train | epoch 006 | loss 7.384 | nll_loss 6.395 | ppl 84.15 | wps 20250.8 | ups 6.76 | wpb 2995.2 | bsz 88.5 | num_updates 8594 | lr 0.000341116 | gnorm 0.959 | loss_scale 8 | train_wall 177 | gb_free 12.5 | wall 1243\n",
            "2024-10-30 16:12:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:12:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 007:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:12:51 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-10-30 16:12:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 1432/1433 [03:07<00:00,  7.66it/s, loss=7.242, nll_loss=6.23, ppl=75.06, wps=22196.7, ups=7.61, wpb=2914.9, bsz=85.6, num_updates=10000, lr=0.000316228, gnorm=1.032, loss_scale=8, train_wall=12, gb_free=12.5, wall=1427]2024-10-30 16:15:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:15:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.54it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 21.49it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 24.02it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 26.66it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 26.75it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 27.36it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  46% 21/46 [00:00<00:00, 28.11it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 24/46 [00:00<00:00, 28.35it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 28/46 [00:01<00:00, 28.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 31/46 [00:01<00:00, 28.34it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  76% 35/46 [00:01<00:00, 30.44it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 39/46 [00:01<00:00, 30.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  93% 43/46 [00:01<00:00, 31.03it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:16:00 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.481 | nll_loss 6.407 | ppl 84.85 | wps 55970.1 | wpb 1880.3 | bsz 57.4 | num_updates 10027 | best_loss 7.481\n",
            "2024-10-30 16:16:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10027 updates\n",
            "2024-10-30 16:16:00 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint7.pt\n",
            "2024-10-30 16:16:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint7.pt\n",
            "2024-10-30 16:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 10027 updates, score 7.481) (writing took 21.051183111000228 seconds)\n",
            "2024-10-30 16:16:21 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-10-30 16:16:21 | INFO | train | epoch 007 | loss 7.225 | nll_loss 6.212 | ppl 74.11 | wps 20355 | ups 6.8 | wpb 2995.2 | bsz 88.5 | num_updates 10027 | lr 0.000315802 | gnorm 0.983 | loss_scale 8 | train_wall 179 | gb_free 12.6 | wall 1454\n",
            "2024-10-30 16:16:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:16:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 008:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:16:22 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-10-30 16:16:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 1432/1433 [03:08<00:00,  8.24it/s, loss=7.106, nll_loss=6.074, ppl=67.35, wps=23143.7, ups=7.61, wpb=3041.3, bsz=105.2, num_updates=11400, lr=0.000296174, gnorm=1.059, loss_scale=8, train_wall=13, gb_free=12.6, wall=1635]2024-10-30 16:19:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:19:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.36it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 22.41it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 25.41it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 27.90it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 28.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  41% 19/46 [00:00<00:00, 29.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 23/46 [00:00<00:00, 29.76it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 27/46 [00:00<00:00, 30.12it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 31/46 [00:01<00:00, 29.02it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 35/46 [00:01<00:00, 30.63it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 39/46 [00:01<00:00, 31.44it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  93% 43/46 [00:01<00:00, 31.69it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:19:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.401 | nll_loss 6.317 | ppl 79.73 | wps 58334.8 | wpb 1880.3 | bsz 57.4 | num_updates 11460 | best_loss 7.401\n",
            "2024-10-30 16:19:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11460 updates\n",
            "2024-10-30 16:19:32 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint8.pt\n",
            "2024-10-30 16:19:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint8.pt\n",
            "2024-10-30 16:19:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 11460 updates, score 7.401) (writing took 24.80850825100015 seconds)\n",
            "2024-10-30 16:19:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-10-30 16:19:57 | INFO | train | epoch 008 | loss 7.095 | nll_loss 6.061 | ppl 66.76 | wps 19893.8 | ups 6.64 | wpb 2995.2 | bsz 88.5 | num_updates 11460 | lr 0.000295398 | gnorm 0.999 | loss_scale 8 | train_wall 180 | gb_free 12.5 | wall 1669\n",
            "2024-10-30 16:19:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:19:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 009:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:19:57 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-10-30 16:19:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 1432/1433 [03:07<00:00,  7.76it/s, loss=7.025, nll_loss=5.979, ppl=63.07, wps=23085.6, ups=7.72, wpb=2989.5, bsz=92.2, num_updates=12800, lr=0.000279508, gnorm=1.089, loss_scale=8, train_wall=12, gb_free=12.6, wall=1845]2024-10-30 16:23:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:23:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.72it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 21.43it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 24.75it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 27.15it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 27.41it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  41% 19/46 [00:00<00:00, 28.89it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 22/46 [00:00<00:00, 28.88it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  57% 26/46 [00:00<00:00, 29.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  63% 29/46 [00:01<00:00, 29.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 32/46 [00:01<00:00, 29.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  78% 36/46 [00:01<00:00, 32.01it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  87% 40/46 [00:01<00:00, 32.44it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  96% 44/46 [00:01<00:00, 32.14it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:23:06 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.335 | nll_loss 6.229 | ppl 75 | wps 57911.1 | wpb 1880.3 | bsz 57.4 | num_updates 12893 | best_loss 7.335\n",
            "2024-10-30 16:23:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12893 updates\n",
            "2024-10-30 16:23:06 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint9.pt\n",
            "2024-10-30 16:23:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint9.pt\n",
            "2024-10-30 16:23:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 12893 updates, score 7.335) (writing took 17.850261230999877 seconds)\n",
            "2024-10-30 16:23:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-10-30 16:23:24 | INFO | train | epoch 009 | loss 6.986 | nll_loss 5.934 | ppl 61.14 | wps 20731.6 | ups 6.92 | wpb 2995.2 | bsz 88.5 | num_updates 12893 | lr 0.000278499 | gnorm 1.022 | loss_scale 8 | train_wall 179 | gb_free 12.6 | wall 1876\n",
            "2024-10-30 16:23:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:23:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1433\n",
            "epoch 010:   0% 0/1433 [00:00<?, ?it/s]2024-10-30 16:23:24 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-10-30 16:23:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 1432/1433 [03:10<00:00,  7.78it/s, loss=6.87, nll_loss=5.8, ppl=55.72, wps=22852.2, ups=7.65, wpb=2988.6, bsz=94.6, num_updates=14300, lr=0.000264443, gnorm=1.085, loss_scale=8, train_wall=12, gb_free=12.5, wall=2064]2024-10-30 16:26:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2024-10-30 16:26:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   4% 2/46 [00:00<00:03, 14.07it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 5/46 [00:00<00:02, 18.93it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 24.03it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 26.66it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 27.39it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  41% 19/46 [00:00<00:00, 28.07it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  48% 22/46 [00:00<00:00, 28.40it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  54% 25/46 [00:00<00:00, 28.55it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  61% 28/46 [00:01<00:00, 28.97it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 31/46 [00:01<00:00, 27.73it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 35/46 [00:01<00:00, 30.00it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 39/46 [00:01<00:00, 31.04it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  93% 43/46 [00:01<00:00, 30.89it/s]\u001b[A\n",
            "                                                                        \u001b[A2024-10-30 16:26:37 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.322 | nll_loss 6.217 | ppl 74.4 | wps 56019.7 | wpb 1880.3 | bsz 57.4 | num_updates 14326 | best_loss 7.322\n",
            "2024-10-30 16:26:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14326 updates\n",
            "2024-10-30 16:26:37 | INFO | fairseq.trainer | Saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint10.pt\n",
            "2024-10-30 16:26:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/fairseq/examples/translation/checkpoints/checkpoint10.pt\n",
            "2024-10-30 16:26:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 14326 updates, score 7.322) (writing took 14.539556833999995 seconds)\n",
            "2024-10-30 16:26:51 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-10-30 16:26:51 | INFO | train | epoch 010 | loss 6.894 | nll_loss 5.828 | ppl 56.79 | wps 20732.4 | ups 6.92 | wpb 2995.2 | bsz 88.5 | num_updates 14326 | lr 0.000264203 | gnorm 1.04 | loss_scale 8 | train_wall 181 | gb_free 13 | wall 2084\n",
            "2024-10-30 16:26:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-10-30 16:26:51 | INFO | fairseq_cli.train | done training in 2082.0 seconds\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
        "    /content/fairseq/examples/translation/data-bin/iwslt14.de-en \\\n",
        "    --max-source-positions 4096 --max-target-positions 4096 \\\n",
        "    --skip-invalid-size-inputs-valid-test \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --keep-last-epochs 2 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --max-epoch 10 \\\n",
        "    --fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4FbCZu9oyvN"
      },
      "source": [
        "# 4. Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVERqf0KoyvN"
      },
      "source": [
        "Now we can generate translations with the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TfwDbYWZoyvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7164d91-6303-4d8b-c663-752c9e30689a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-30 16:27:10.020568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-30 16:27:10.057867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-30 16:27:10.069934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-30 16:27:10.094893: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-30 16:27:11.863687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/fairseq/examples/translation/checkpoints/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/content/fairseq/examples/translation/data-bin/iwslt14.de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 4096, 'max_target_positions': 4096, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "INFO:fairseq.tasks.translation:[de] dictionary: 11512 types\n",
            "INFO:fairseq.tasks.translation:[en] dictionary: 10600 types\n",
            "INFO:fairseq_cli.generate:loading model(s) from /content/fairseq/examples/translation/checkpoints/checkpoint_best.pt\n",
            "/content/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "INFO:fairseq.data.data_utils:loaded 2,642 examples from: /content/fairseq/examples/translation/data-bin/iwslt14.de-en/test.de-en.de\n",
            "INFO:fairseq.data.data_utils:loaded 2,642 examples from: /content/fairseq/examples/translation/data-bin/iwslt14.de-en/test.de-en.en\n",
            "INFO:fairseq.tasks.translation:/content/fairseq/examples/translation/data-bin/iwslt14.de-en test de-en 2642 examples\n",
            "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True\n",
            "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
            "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
            "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n",
            "INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2\n",
            "INFO:fairseq_cli.generate:Translated 2,640 sentences (34,866 tokens) in 30.7s (86.13 sentences/s, 1137.52 tokens/s)\n"
          ]
        }
      ],
      "source": [
        "# TEST_INPUT=\"/content/fairseq/examples/translation/sample_data/spm.tst.de-en.de\"\n",
        "PRED_LOG=\"/content/fairseq/examples/translation/de-en.decode.log\"\n",
        "\n",
        "!fairseq-generate /content/fairseq/examples/translation/data-bin/iwslt14.de-en \\\n",
        "      --task translation \\\n",
        "      --source-lang de \\\n",
        "      --target-lang en \\\n",
        "      --path /content/fairseq/examples/translation/checkpoints/checkpoint_best.pt \\\n",
        "      --batch-size 256 \\\n",
        "      --beam 4 \\\n",
        "      --max-source-positions 4096 --max-target-positions 4096 \\\n",
        "      --skip-invalid-size-inputs-valid-test \\\n",
        "      --remove-bpe=sentencepiece > $PRED_LOG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY5ZbTnxoyvN"
      },
      "source": [
        "We extract the hypotheses and references from the decoding log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nNf_RDPBoyvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b799504b-ee72-47cc-f9aa-8d2e61d2cf9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discography\n",
            "Background\n",
            "Personal life\n",
            "Discography\n",
            "Family\n",
            "Education\n",
            "Gallery\n",
            "Education\n",
            "Location\n",
            "Examples\n",
            "\n",
            "Publications (selection)\n",
            "Column connectors\n",
            "Teaching and Publications\n",
            "Publications (selection)\n",
            "Notable family members\n",
            "Training and operations manager\n",
            "New beginning 1929\n",
            "Youth and professional life\n",
            "Rhyme structure\n",
            "Calendar Definition\n"
          ]
        }
      ],
      "source": [
        "!grep ^H \"de-en.decode.log\" | sed 's/^H-//g' | cut -f 3 | sed 's/ ##//g' > ./hyp.txt\n",
        "!grep ^T \"de-en.decode.log\" | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > ./ref.txt\n",
        "!head ./hyp.txt\n",
        "!echo \"\"\n",
        "!head ./ref.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtSSbrOBoyvN"
      },
      "source": [
        "# Section 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hdfgdT24oyvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6a3045-025e-4156-c920-a37db07a0efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 0.5,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
            " \"verbose_score\": \"19.5/2.4/0.6/0.2 (BP = 0.297 ratio = 0.452 hyp_len = 27473 ref_len = 60809)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.4.3\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!bash -c \"cat hyp.txt | sacrebleu ref.txt\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}