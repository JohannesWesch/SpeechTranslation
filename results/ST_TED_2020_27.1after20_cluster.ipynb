{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install pip and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('compiler/gnu/13.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv/myEnv/bin/python\n",
      "['/pfs/data5/home/kit/stud/unyfv', '', '/home/kit/stud/unyfv/.local/lib/python3.9/site-packages', '/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages', '/usr/lib64/python39.zip', '/usr/lib64/python3.9', '/usr/lib64/python3.9/lib-dynload', '/pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages', '/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.executable 应该指向你的虚拟环境中的 Python 解释器。\n",
    "# sys.path 中应该包含你虚拟环境的 site-packages 目录。\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv/myEnv/bin/python\n",
      "/pfs/data5/home/kit/stud/unyfv/myEnv/bin/pip\n",
      "/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 更新 PYTHONPATH 和 PATH\n",
    "os.environ[\"PYTHONPATH\"] = \"/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PATH\"] = \"/pfs/data5/home/kit/stud/unyfv/myEnv/bin:\" + os.environ[\"PATH\"]\n",
    "# 验证更新\n",
    "!which python\n",
    "!which pip\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip==24.0 in ./myEnv/lib/python3.9/site-packages (24.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pip==24.0\n",
    "!pip show torch | grep Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Install fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install fairseq, trust me, the original installation method sucks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fairseq' already exists and is not an empty directory.\n",
      "/pfs/data5/home/kit/stud/unyfv/fairseq\n",
      "Obtaining file:///pfs/data5/home/kit/stud/unyfv/fairseq\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cffi in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (1.17.1)\n",
      "Requirement already satisfied: cython in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (3.0.11)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from fairseq==0.12.2) (1.0.7)\n",
      "Requirement already satisfied: omegaconf<2.1 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from fairseq==0.12.2) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.21.3 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (2.0.2)\n",
      "Requirement already satisfied: regex in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (2024.9.11)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from fairseq==0.12.2) (2.4.3)\n",
      "Requirement already satisfied: torch>=1.13 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from fairseq==0.12.2) (4.66.6)\n",
      "Requirement already satisfied: bitarray in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (3.0.0)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from fairseq==0.12.2) (1.5.2)\n",
      "Requirement already satisfied: packaging in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from fairseq==0.12.2) (24.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from omegaconf<2.1->fairseq==0.12.2) (4.12.2)\n",
      "Requirement already satisfied: portalocker in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.10.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
      "Requirement already satisfied: colorama in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
      "Requirement already satisfied: lxml in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (5.3.0)\n",
      "Requirement already satisfied: filelock in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (3.16.1)\n",
      "Requirement already satisfied: networkx in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from torch>=1.13->fairseq==0.12.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from cffi->fairseq==0.12.2) (2.22)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from scikit-learn->fairseq==0.12.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from scikit-learn->fairseq==0.12.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (3.0.2)\n",
      "Building wheels for collected packages: fairseq\n",
      "  Building editable for fairseq (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp39-cp39-linux_x86_64.whl size=9573 sha256=c64f4be617890d0fec28b1628d602597e4a6ebe303d24ae737c0c873124bebe8\n",
      "  Stored in directory: /scratch/slurm_tmpdir/job_24586128/pip-ephem-wheel-cache-bh_kh7nb/wheels/04/f3/80/b3eeb7f36a7a1c71a666614a9545aa3a57c1e9d004ff1e7236\n",
      "Successfully built fairseq\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: fairseq\n",
      "  Attempting uninstall: fairseq\n",
      "    Found existing installation: fairseq 0.12.2\n",
      "    Not uninstalling fairseq at /pfs/data5/home/kit/stud/unyfv/fairseq, outside environment /pfs/data5/home/kit/stud/unyfv/myEnv\n",
      "    Can't uninstall 'fairseq'. No files were found to uninstall.\n",
      "Successfully installed fairseq-0.12.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/fairseq.git\n",
    "%cd fairseq\n",
    "!pip install --editable ./ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to add a new environment variable so that we can use the fairseq command in the terminal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages:\n",
      "/pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages::/pfs/data5/home/kit/stud/unyfv/fairseq/\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH\n",
    "os.environ['PYTHONPATH'] += \":/pfs/data5/home/kit/stud/unyfv/fairseq/\"\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Install other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from sacremoses) (2024.9.11)\n",
      "Requirement already satisfied: click in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacremoses) (4.66.6)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (0.2.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sacrebleu in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (2.4.3)\n",
      "Requirement already satisfied: portalocker in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacrebleu) (2.10.1)\n",
      "Requirement already satisfied: regex in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from sacrebleu) (2024.9.11)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from sacrebleu) (2.0.2)\n",
      "Requirement already satisfied: colorama in /pfs/data5/home/kit/stud/unyfv/myEnv/lib/python3.9/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages (from sacrebleu) (5.3.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses\n",
    "!pip install sentencepiece\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Activate GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "Current device:  Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device(); print('Current device: ', torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    device = 'cpu'; print('Current device: CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all packages have been installed.\n",
    "From now on, just execute the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we still use TED-dataset as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-01 14:58:27--  https://bwsyncandshare.kit.edu/s/Xx3D56SJmG8PwXj/download\n",
      "Resolving bwsyncandshare.kit.edu (bwsyncandshare.kit.edu)... 2a00:1398:b::8d03:8781, 141.3.135.129\n",
      "Connecting to bwsyncandshare.kit.edu (bwsyncandshare.kit.edu)|2a00:1398:b::8d03:8781|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘sample_data.zip’\n",
      "\n",
      "sample_data.zip         [     <=>            ]  28.19M  23.2MB/s    in 1.2s    \n",
      "\n",
      "2024-11-01 14:58:29 (23.2 MB/s) - ‘sample_data.zip’ saved [29555676]\n",
      "\n",
      "Archive:  sample_data.zip\n",
      "   creating: dataset/sample_data/\n",
      " extracting: dataset/sample_data/dev.de-en.de  \n",
      " extracting: dataset/sample_data/dev.de-en.en  \n",
      " extracting: dataset/sample_data/train.de-en.de  \n",
      " extracting: dataset/sample_data/train.de-en.en  \n",
      " extracting: dataset/sample_data/tst.de-en.de  \n",
      " extracting: dataset/sample_data/tst.de-en.en  \n"
     ]
    }
   ],
   "source": [
    "!wget -nc -O sample_data.zip https://bwsyncandshare.kit.edu/s/Xx3D56SJmG8PwXj/download\n",
    "!unzip sample_data.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment the text into subwords using BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training sentencepiece model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: dataset/sample_data/train.de-en.en\n",
      "  input: dataset/sample_data/train.de-en.de\n",
      "  input_format: \n",
      "  model_prefix: bpe\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: dataset/sample_data/train.de-en.en\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: dataset/sample_data/train.de-en.de\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 401184 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=27407358\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9511% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=75\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999511\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 401184 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=14107737\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 460005 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 401184\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 265594\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 265594 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=127458 obj=11.9096 num_tokens=550629 num_tokens/piece=4.32008\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=109100 obj=9.3836 num_tokens=552841 num_tokens/piece=5.06729\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=81787 obj=9.37685 num_tokens=586522 num_tokens/piece=7.17134\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=81727 obj=9.36154 num_tokens=586622 num_tokens/piece=7.17782\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=61292 obj=9.45883 num_tokens=635836 num_tokens/piece=10.3739\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=61286 obj=9.44005 num_tokens=635796 num_tokens/piece=10.3742\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45962 obj=9.56859 num_tokens=690791 num_tokens/piece=15.0296\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45962 obj=9.54545 num_tokens=690774 num_tokens/piece=15.0292\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=34470 obj=9.70494 num_tokens=749072 num_tokens/piece=21.7311\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=34470 obj=9.6788 num_tokens=748979 num_tokens/piece=21.7284\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25852 obj=9.86473 num_tokens=808078 num_tokens/piece=31.2579\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25852 obj=9.83449 num_tokens=808018 num_tokens/piece=31.2555\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19389 obj=10.0529 num_tokens=867766 num_tokens/piece=44.7556\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19389 obj=10.018 num_tokens=867693 num_tokens/piece=44.7518\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14541 obj=10.2719 num_tokens=928425 num_tokens/piece=63.8488\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14541 obj=10.2311 num_tokens=928429 num_tokens/piece=63.849\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=10.5084 num_tokens=985893 num_tokens/piece=89.6266\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=10.4634 num_tokens=985871 num_tokens/piece=89.6246\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# After execution, you can find two bpe files in the directory.\n",
    "spm.SentencePieceTrainer.train(input=\"dataset/sample_data/train.de-en.en,dataset/sample_data/train.de-en.de\",\n",
    "                               model_prefix=\"bpe\",\n",
    "                               vocab_size=10000)\n",
    "\n",
    "print('Finished training sentencepiece model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the trained segmentation model to preprocess the sentences from train/dev/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained sentencepiece model\n",
    "spm_model = spm.SentencePieceProcessor(model_file=\"bpe.model\")\n",
    "\n",
    "# Important: encoding=\"utf-8\"\n",
    "for partition in [\"train\", \"dev\", \"tst\"]:\n",
    "    for lang in [\"de\", \"en\"]:\n",
    "        f_out = open(f\"dataset/sample_data/spm.{partition}.de-en.{lang}\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "        with open(f\"dataset/sample_data/{partition}.de-en.{lang}\", \"r\", encoding=\"utf-8\") as f_in:\n",
    "            for line_idx, line in enumerate(f_in.readlines()):\n",
    "                # Segmented into subwords\n",
    "                line_segmented = spm_model.encode(line.strip(), out_type=str)\n",
    "                # Join the subwords into a string\n",
    "                line_segmented = \" \".join(line_segmented)\n",
    "                f_out.write(line_segmented + \"\\n\")\n",
    "\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will binarize the data for training with fairseq.  \n",
    "Feel free to check the [documentation](https://fairseq.readthedocs.io/en/latest/command_line_tools.html) of fairseq commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/sample_data\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='en', target_lang='de', trainpref='dataset/sample_data/spm.train.de-en', validpref='dataset/sample_data/spm.dev.de-en', testpref='dataset/sample_data/spm.tst.de-en', align_suffix=None, destdir='binarized_data/iwslt14.de-en', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 6080 types\n",
      "INFO:fairseq_cli.preprocess:[en] dataset/sample_data/spm.train.de-en.en: 200592 sents, 3665856 tokens, 0.0% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 6080 types\n",
      "INFO:fairseq_cli.preprocess:[en] dataset/sample_data/spm.dev.de-en.en: 4179 sents, 74674 tokens, 0.00402% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 6080 types\n",
      "INFO:fairseq_cli.preprocess:[en] dataset/sample_data/spm.tst.de-en.en: 4179 sents, 72921 tokens, 0.00823% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 7448 types\n",
      "INFO:fairseq_cli.preprocess:[de] dataset/sample_data/spm.train.de-en.de: 200592 sents, 3584781 tokens, 0.0% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 7448 types\n",
      "INFO:fairseq_cli.preprocess:[de] dataset/sample_data/spm.dev.de-en.de: 4179 sents, 74522 tokens, 0.0134% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 7448 types\n",
      "INFO:fairseq_cli.preprocess:[de] dataset/sample_data/spm.tst.de-en.de: 4179 sents, 73200 tokens, 0.0041% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:Wrote preprocessed data to binarized_data/iwslt14.de-en\n"
     ]
    }
   ],
   "source": [
    "# Preprocess/binarize the data\n",
    "TEXT=\"dataset/sample_data\"\n",
    "!echo $TEXT\n",
    "# Binarize the data for training\n",
    "!fairseq-preprocess \\\n",
    "    --source-lang en --target-lang de \\\n",
    "    --trainpref $TEXT/spm.train.de-en \\\n",
    "    --validpref $TEXT/spm.dev.de-en \\\n",
    "    --testpref $TEXT/spm.tst.de-en \\\n",
    "    --destdir binarized_data/iwslt14.de-en \\\n",
    "    --thresholdtgt 0 --thresholdsrc 0 \\\n",
    "    --workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preprocessing is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 15:02:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-11-01 15:02:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=20, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='binarized_data/iwslt14.de-en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'binarized_data/iwslt14.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "2024-11-01 15:02:17 | INFO | fairseq.tasks.translation | [en] dictionary: 6080 types\n",
      "2024-11-01 15:02:17 | INFO | fairseq.tasks.translation | [de] dictionary: 7448 types\n",
      "2024-11-01 15:02:20 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(6080, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(7448, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=7448, bias=False)\n",
      "  )\n",
      ")\n",
      "2024-11-01 15:02:20 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2024-11-01 15:02:20 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2024-11-01 15:02:20 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2024-11-01 15:02:20 | INFO | fairseq_cli.train | num. shared model params: 51,064,832 (num. trained: 51,064,832)\n",
      "2024-11-01 15:02:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2024-11-01 15:02:20 | INFO | fairseq.data.data_utils | loaded 4,179 examples from: binarized_data/iwslt14.de-en/valid.en-de.en\n",
      "2024-11-01 15:02:20 | INFO | fairseq.data.data_utils | loaded 4,179 examples from: binarized_data/iwslt14.de-en/valid.en-de.de\n",
      "2024-11-01 15:02:20 | INFO | fairseq.tasks.translation | binarized_data/iwslt14.de-en valid en-de 4179 examples\n",
      "2024-11-01 15:02:21 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2024-11-01 15:02:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2024-11-01 15:02:21 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.733 GB ; name = Tesla V100-SXM2-32GB                    \n",
      "2024-11-01 15:02:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2024-11-01 15:02:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2024-11-01 15:02:21 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2024-11-01 15:02:21 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "2024-11-01 15:02:21 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
      "2024-11-01 15:02:21 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2024-11-01 15:02:21 | INFO | fairseq.data.data_utils | loaded 200,592 examples from: binarized_data/iwslt14.de-en/train.en-de.en\n",
      "2024-11-01 15:02:21 | INFO | fairseq.data.data_utils | loaded 200,592 examples from: binarized_data/iwslt14.de-en/train.en-de.de\n",
      "2024-11-01 15:02:21 | INFO | fairseq.tasks.translation | binarized_data/iwslt14.de-en train en-de 200592 examples\n",
      "2024-11-01 15:02:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:02:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-11-01 15:02:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-11-01 15:02:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2024-11-01 15:02:33 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2024-11-01 15:02:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:02:33 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2024-11-01 15:02:33 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2024-11-01 15:02:33 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2024-11-01 15:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 001:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:02:34 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2024-11-01 15:02:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/pfs/data5/home/kit/stud/unyfv/fairseq/fairseq/tasks/fairseq_task.py:531: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):\n",
      "2024-11-01 15:02:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                             | 1/1008 [00:08<2:22:44,  8.51s/it]/pfs/data5/home/kit/stud/unyfv/myEnv/lib64/python3.9/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "epoch 001:  32%|▎| 318/1008 [00:27<00:36, 18.68it/s, loss=10.108, nll_loss=9.6772024-11-01 15:03:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001: 100%|▉| 1007/1008 [01:04<00:00, 18.97it/s, loss=8.732, nll_loss=8.0452024-11-01 15:03:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:03:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 54.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 60.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 62.24it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 66.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:03:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.515 | nll_loss 7.752 | ppl 215.56 | wps 158706 | wpb 2325.1 | bsz 130.5 | num_updates 1006\n",
      "2024-11-01 15:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1006 updates\n",
      "2024-11-01 15:03:39 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint1.pt\n",
      "2024-11-01 15:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint1.pt\n",
      "2024-11-01 15:03:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 1006 updates, score 8.515) (writing took 3.124943318311125 seconds)\n",
      "2024-11-01 15:03:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2024-11-01 15:03:42 | INFO | train | epoch 001 | loss 9.784 | nll_loss 9.269 | ppl 617.1 | wps 61114.3 | ups 17.18 | wpb 3556.6 | bsz 198 | num_updates 1006 | lr 0.00012575 | gnorm 1.851 | loss_scale 32 | train_wall 62 | gb_free 29.7 | wall 82\n",
      "2024-11-01 15:03:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:03:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 002:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:03:42 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2024-11-01 15:03:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  90%|▉| 909/1008 [00:48<00:05, 18.56it/s, loss=7.788, nll_loss=6.941,2024-11-01 15:04:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 002: 100%|▉| 1006/1008 [00:54<00:00, 19.17it/s, loss=7.905, nll_loss=7.0682024-11-01 15:04:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:04:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 56.05it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 62.22it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 63.88it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:04:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.477 | nll_loss 6.536 | ppl 92.8 | wps 161765 | wpb 2325.1 | bsz 130.5 | num_updates 2013 | best_loss 7.477\n",
      "2024-11-01 15:04:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2013 updates\n",
      "2024-11-01 15:04:37 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint2.pt\n",
      "2024-11-01 15:04:38 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint2.pt\n",
      "2024-11-01 15:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 2013 updates, score 7.477) (writing took 3.191727336961776 seconds)\n",
      "2024-11-01 15:04:40 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2024-11-01 15:04:40 | INFO | train | epoch 002 | loss 8.143 | nll_loss 7.354 | ppl 163.56 | wps 61776.9 | ups 17.37 | wpb 3556.3 | bsz 198.2 | num_updates 2013 | lr 0.000251625 | gnorm 1.401 | loss_scale 16 | train_wall 52 | gb_free 29.8 | wall 139\n",
      "2024-11-01 15:04:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:04:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 003:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:04:40 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2024-11-01 15:04:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003: 100%|▉| 1006/1008 [00:54<00:00, 19.28it/s, loss=6.893, nll_loss=5.8972024-11-01 15:05:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:05:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 57.57it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 63.17it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 64.09it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:05:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.785 | nll_loss 5.727 | ppl 52.96 | wps 162360 | wpb 2325.1 | bsz 130.5 | num_updates 3021 | best_loss 6.785\n",
      "2024-11-01 15:05:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3021 updates\n",
      "2024-11-01 15:05:35 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint3.pt\n",
      "2024-11-01 15:05:36 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint3.pt\n",
      "2024-11-01 15:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 3021 updates, score 6.785) (writing took 3.195893521886319 seconds)\n",
      "2024-11-01 15:05:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2024-11-01 15:05:38 | INFO | train | epoch 003 | loss 7.292 | nll_loss 6.361 | ppl 82.19 | wps 61604.8 | ups 17.32 | wpb 3556.3 | bsz 199 | num_updates 3021 | lr 0.000377625 | gnorm 1.211 | loss_scale 16 | train_wall 52 | gb_free 29.9 | wall 198\n",
      "2024-11-01 15:05:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:05:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 004:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:05:39 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2024-11-01 15:05:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004: 100%|▉| 1006/1008 [00:54<00:00, 18.96it/s, loss=6.284, nll_loss=5.1852024-11-01 15:06:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:06:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 56.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 61.57it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 62.80it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 66.67it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:06:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.969 | nll_loss 4.731 | ppl 26.56 | wps 160379 | wpb 2325.1 | bsz 130.5 | num_updates 4029 | best_loss 5.969\n",
      "2024-11-01 15:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4029 updates\n",
      "2024-11-01 15:06:33 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint4.pt\n",
      "2024-11-01 15:06:34 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint4.pt\n",
      "2024-11-01 15:06:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 4029 updates, score 5.969) (writing took 3.179433710873127 seconds)\n",
      "2024-11-01 15:06:37 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2024-11-01 15:06:37 | INFO | train | epoch 004 | loss 6.553 | nll_loss 5.5 | ppl 45.24 | wps 61731.1 | ups 17.36 | wpb 3556.3 | bsz 199 | num_updates 4029 | lr 0.000498197 | gnorm 1.176 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 256\n",
      "2024-11-01 15:06:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:06:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 005:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:06:37 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2024-11-01 15:06:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005: 100%|▉| 1005/1008 [00:54<00:00, 18.94it/s, loss=5.578, nll_loss=4.3712024-11-01 15:07:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:07:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 54.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 60.06it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 61.48it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 65.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:07:31 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.326 | nll_loss 3.978 | ppl 15.75 | wps 157698 | wpb 2325.1 | bsz 130.5 | num_updates 5037 | best_loss 5.326\n",
      "2024-11-01 15:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5037 updates\n",
      "2024-11-01 15:07:31 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint5.pt\n",
      "2024-11-01 15:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint5.pt\n",
      "2024-11-01 15:07:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 5037 updates, score 5.326) (writing took 3.2017496922053397 seconds)\n",
      "2024-11-01 15:07:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2024-11-01 15:07:35 | INFO | train | epoch 005 | loss 5.828 | nll_loss 4.658 | ppl 25.25 | wps 61596.3 | ups 17.32 | wpb 3556.3 | bsz 199 | num_updates 5037 | lr 0.000445568 | gnorm 1.117 | loss_scale 16 | train_wall 52 | gb_free 29.6 | wall 314\n",
      "2024-11-01 15:07:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:07:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 006:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:07:35 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2024-11-01 15:07:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006: 100%|▉| 1007/1008 [00:54<00:00, 18.92it/s, loss=5.197, nll_loss=3.9312024-11-01 15:08:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:08:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  16%|█      | 5/32 [00:00<00:00, 43.87it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▎   | 12/32 [00:00<00:00, 55.45it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  59%|███▌  | 19/32 [00:00<00:00, 59.70it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  84%|█████ | 27/32 [00:00<00:00, 64.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:08:29 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.051 | nll_loss 3.639 | ppl 12.46 | wps 155254 | wpb 2325.1 | bsz 130.5 | num_updates 6045 | best_loss 5.051\n",
      "2024-11-01 15:08:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6045 updates\n",
      "2024-11-01 15:08:29 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint6.pt\n",
      "2024-11-01 15:08:30 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint6.pt\n",
      "2024-11-01 15:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 6045 updates, score 5.051) (writing took 3.2312746942043304 seconds)\n",
      "2024-11-01 15:08:33 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2024-11-01 15:08:33 | INFO | train | epoch 006 | loss 5.305 | nll_loss 4.055 | ppl 16.62 | wps 61738 | ups 17.36 | wpb 3556.3 | bsz 199 | num_updates 6045 | lr 0.000406726 | gnorm 1.106 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 372\n",
      "2024-11-01 15:08:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:08:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 007:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:08:33 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2024-11-01 15:08:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007: 100%|▉| 1006/1008 [00:54<00:00, 18.74it/s, loss=4.847, nll_loss=3.5322024-11-01 15:09:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:09:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 54.44it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 60.79it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 62.03it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 66.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:09:28 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.839 | nll_loss 3.392 | ppl 10.5 | wps 158784 | wpb 2325.1 | bsz 130.5 | num_updates 7053 | best_loss 4.839\n",
      "2024-11-01 15:09:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7053 updates\n",
      "2024-11-01 15:09:28 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint7.pt\n",
      "2024-11-01 15:09:28 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint7.pt\n",
      "2024-11-01 15:09:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 7053 updates, score 4.839) (writing took 3.23117044987157 seconds)\n",
      "2024-11-01 15:09:31 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2024-11-01 15:09:31 | INFO | train | epoch 007 | loss 4.963 | nll_loss 3.663 | ppl 12.66 | wps 61730.6 | ups 17.36 | wpb 3556.3 | bsz 199 | num_updates 7053 | lr 0.000376542 | gnorm 1.082 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 430\n",
      "2024-11-01 15:09:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:09:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 008:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:09:31 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2024-11-01 15:09:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008: 100%|▉| 1005/1008 [00:54<00:00, 18.90it/s, loss=4.712, nll_loss=3.3762024-11-01 15:10:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:10:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  16%|█      | 5/32 [00:00<00:00, 43.54it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▎   | 12/32 [00:00<00:00, 56.48it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  59%|███▌  | 19/32 [00:00<00:00, 60.74it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  84%|█████ | 27/32 [00:00<00:00, 65.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:10:26 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.692 | nll_loss 3.227 | ppl 9.36 | wps 156515 | wpb 2325.1 | bsz 130.5 | num_updates 8061 | best_loss 4.692\n",
      "2024-11-01 15:10:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8061 updates\n",
      "2024-11-01 15:10:26 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint8.pt\n",
      "2024-11-01 15:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint8.pt\n",
      "2024-11-01 15:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 8061 updates, score 4.692) (writing took 3.2136791800148785 seconds)\n",
      "2024-11-01 15:10:29 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2024-11-01 15:10:29 | INFO | train | epoch 008 | loss 4.723 | nll_loss 3.389 | ppl 10.48 | wps 61452.7 | ups 17.28 | wpb 3556.3 | bsz 199 | num_updates 8061 | lr 0.000352213 | gnorm 1.067 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 488\n",
      "2024-11-01 15:10:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:10:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 009:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:10:29 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2024-11-01 15:10:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009: 100%|▉| 1006/1008 [00:54<00:00, 19.08it/s, loss=4.527, nll_loss=3.1682024-11-01 15:11:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:11:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 55.57it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 61.81it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 63.19it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 66.98it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:11:24 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.589 | nll_loss 3.115 | ppl 8.66 | wps 161209 | wpb 2325.1 | bsz 130.5 | num_updates 9069 | best_loss 4.589\n",
      "2024-11-01 15:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9069 updates\n",
      "2024-11-01 15:11:24 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint9.pt\n",
      "2024-11-01 15:11:25 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint9.pt\n",
      "2024-11-01 15:11:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 9069 updates, score 4.589) (writing took 3.2146754721179605 seconds)\n",
      "2024-11-01 15:11:27 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2024-11-01 15:11:27 | INFO | train | epoch 009 | loss 4.54 | nll_loss 3.181 | ppl 9.07 | wps 61605.5 | ups 17.32 | wpb 3556.3 | bsz 199 | num_updates 9069 | lr 0.000332063 | gnorm 1.046 | loss_scale 16 | train_wall 52 | gb_free 29.8 | wall 547\n",
      "2024-11-01 15:11:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:11:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 010:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:11:27 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2024-11-01 15:11:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010: 100%|▉| 1006/1008 [00:54<00:00, 19.17it/s, loss=4.373, nll_loss=2.9922024-11-01 15:12:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:12:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 55.21it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 60.67it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 61.28it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 65.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:12:22 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.485 | nll_loss 3.005 | ppl 8.03 | wps 158524 | wpb 2325.1 | bsz 130.5 | num_updates 10077 | best_loss 4.485\n",
      "2024-11-01 15:12:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 10077 updates\n",
      "2024-11-01 15:12:22 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint10.pt\n",
      "2024-11-01 15:12:23 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint10.pt\n",
      "2024-11-01 15:12:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 10077 updates, score 4.485) (writing took 3.2018992882221937 seconds)\n",
      "2024-11-01 15:12:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2024-11-01 15:12:26 | INFO | train | epoch 010 | loss 4.398 | nll_loss 3.019 | ppl 8.11 | wps 61589.4 | ups 17.32 | wpb 3556.3 | bsz 199 | num_updates 10077 | lr 0.000315017 | gnorm 1.033 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 605\n",
      "2024-11-01 15:12:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:12:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 011:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:12:26 | INFO | fairseq.trainer | begin training epoch 11\n",
      "2024-11-01 15:12:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 011: 100%|▉| 1006/1008 [00:54<00:00, 19.15it/s, loss=4.299, nll_loss=2.9082024-11-01 15:13:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:13:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 57.15it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 62.92it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 64.29it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:13:20 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.432 | nll_loss 2.935 | ppl 7.65 | wps 162204 | wpb 2325.1 | bsz 130.5 | num_updates 11085 | best_loss 4.432\n",
      "2024-11-01 15:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 11085 updates\n",
      "2024-11-01 15:13:20 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint11.pt\n",
      "2024-11-01 15:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint11.pt\n",
      "2024-11-01 15:13:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint11.pt (epoch 11 @ 11085 updates, score 4.432) (writing took 3.1865962999872863 seconds)\n",
      "2024-11-01 15:13:24 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2024-11-01 15:13:24 | INFO | train | epoch 011 | loss 4.282 | nll_loss 2.889 | ppl 7.41 | wps 61753.2 | ups 17.36 | wpb 3556.3 | bsz 199 | num_updates 11085 | lr 0.000300353 | gnorm 1.028 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 663\n",
      "2024-11-01 15:13:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:13:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 012:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:13:24 | INFO | fairseq.trainer | begin training epoch 12\n",
      "2024-11-01 15:13:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 012: 100%|▉| 1006/1008 [00:54<00:00, 19.07it/s, loss=4.237, nll_loss=2.8372024-11-01 15:14:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:14:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 56.60it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 62.71it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 64.10it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.61it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:14:19 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.391 | nll_loss 2.886 | ppl 7.39 | wps 162887 | wpb 2325.1 | bsz 130.5 | num_updates 12093 | best_loss 4.391\n",
      "2024-11-01 15:14:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 12093 updates\n",
      "2024-11-01 15:14:19 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint12.pt\n",
      "2024-11-01 15:14:19 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint12.pt\n",
      "2024-11-01 15:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint12.pt (epoch 12 @ 12093 updates, score 4.391) (writing took 3.199600565712899 seconds)\n",
      "2024-11-01 15:14:22 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2024-11-01 15:14:22 | INFO | train | epoch 012 | loss 4.187 | nll_loss 2.781 | ppl 6.87 | wps 61624.7 | ups 17.33 | wpb 3556.3 | bsz 199 | num_updates 12093 | lr 0.000287563 | gnorm 1.025 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 721\n",
      "2024-11-01 15:14:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:14:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 013:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:14:22 | INFO | fairseq.trainer | begin training epoch 13\n",
      "2024-11-01 15:14:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 013: 100%|▉| 1007/1008 [00:54<00:00, 18.76it/s, loss=4.107, nll_loss=2.69,2024-11-01 15:15:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:15:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 53.68it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 58.13it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 59.99it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 64.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:15:17 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.373 | nll_loss 2.871 | ppl 7.32 | wps 153813 | wpb 2325.1 | bsz 130.5 | num_updates 13101 | best_loss 4.373\n",
      "2024-11-01 15:15:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 13101 updates\n",
      "2024-11-01 15:15:17 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint13.pt\n",
      "2024-11-01 15:15:18 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint13.pt\n",
      "2024-11-01 15:15:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint13.pt (epoch 13 @ 13101 updates, score 4.373) (writing took 3.2690865811891854 seconds)\n",
      "2024-11-01 15:15:20 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2024-11-01 15:15:20 | INFO | train | epoch 013 | loss 4.102 | nll_loss 2.685 | ppl 6.43 | wps 61481.2 | ups 17.29 | wpb 3556.3 | bsz 199 | num_updates 13101 | lr 0.000276279 | gnorm 1.019 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 779\n",
      "2024-11-01 15:15:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 014:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:15:20 | INFO | fairseq.trainer | begin training epoch 14\n",
      "2024-11-01 15:15:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 014: 100%|▉| 1007/1008 [00:54<00:00, 19.25it/s, loss=4.052, nll_loss=2.6292024-11-01 15:16:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:16:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 56.49it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 62.80it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 64.25it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.52it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:16:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.333 | nll_loss 2.822 | ppl 7.07 | wps 162558 | wpb 2325.1 | bsz 130.5 | num_updates 14109 | best_loss 4.333\n",
      "2024-11-01 15:16:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 14109 updates\n",
      "2024-11-01 15:16:15 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint14.pt\n",
      "2024-11-01 15:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint14.pt\n",
      "2024-11-01 15:16:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint14.pt (epoch 14 @ 14109 updates, score 4.333) (writing took 3.211755181197077 seconds)\n",
      "2024-11-01 15:16:18 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2024-11-01 15:16:18 | INFO | train | epoch 014 | loss 4.03 | nll_loss 2.602 | ppl 6.07 | wps 61530.5 | ups 17.3 | wpb 3556.3 | bsz 199 | num_updates 14109 | lr 0.000266227 | gnorm 1.022 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 838\n",
      "2024-11-01 15:16:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:16:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 015:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:16:18 | INFO | fairseq.trainer | begin training epoch 15\n",
      "2024-11-01 15:16:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 015: 100%|▉| 1007/1008 [00:54<00:00, 19.02it/s, loss=4.02, nll_loss=2.593,2024-11-01 15:17:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:17:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 55.43it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 59.66it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 60.83it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 65.51it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:17:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.312 | nll_loss 2.798 | ppl 6.95 | wps 157300 | wpb 2325.1 | bsz 130.5 | num_updates 15117 | best_loss 4.312\n",
      "2024-11-01 15:17:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 15117 updates\n",
      "2024-11-01 15:17:13 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint15.pt\n",
      "2024-11-01 15:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint15.pt\n",
      "2024-11-01 15:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint15.pt (epoch 15 @ 15117 updates, score 4.312) (writing took 3.198138741776347 seconds)\n",
      "2024-11-01 15:17:17 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2024-11-01 15:17:17 | INFO | train | epoch 015 | loss 3.967 | nll_loss 2.531 | ppl 5.78 | wps 61429.5 | ups 17.27 | wpb 3556.3 | bsz 199 | num_updates 15117 | lr 0.000257198 | gnorm 1.026 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 896\n",
      "2024-11-01 15:17:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:17:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 016:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:17:17 | INFO | fairseq.trainer | begin training epoch 16\n",
      "2024-11-01 15:17:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 016: 100%|▉| 1007/1008 [00:54<00:00, 18.80it/s, loss=3.961, nll_loss=2.5262024-11-01 15:18:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:18:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 56.15it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  38%|██▎   | 12/32 [00:00<00:00, 58.10it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  59%|███▌  | 19/32 [00:00<00:00, 60.86it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  84%|█████ | 27/32 [00:00<00:00, 65.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:18:12 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.295 | nll_loss 2.777 | ppl 6.85 | wps 157969 | wpb 2325.1 | bsz 130.5 | num_updates 16125 | best_loss 4.295\n",
      "2024-11-01 15:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 16125 updates\n",
      "2024-11-01 15:18:12 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint16.pt\n",
      "2024-11-01 15:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint16.pt\n",
      "2024-11-01 15:18:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint16.pt (epoch 16 @ 16125 updates, score 4.295) (writing took 3.2251433301717043 seconds)\n",
      "2024-11-01 15:18:15 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2024-11-01 15:18:15 | INFO | train | epoch 016 | loss 3.909 | nll_loss 2.464 | ppl 5.52 | wps 61527.8 | ups 17.3 | wpb 3556.3 | bsz 199 | num_updates 16125 | lr 0.000249029 | gnorm 1.026 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 954\n",
      "2024-11-01 15:18:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:18:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 017:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:18:15 | INFO | fairseq.trainer | begin training epoch 17\n",
      "2024-11-01 15:18:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 017: 100%|▉| 1007/1008 [00:54<00:00, 19.00it/s, loss=3.922, nll_loss=2.4812024-11-01 15:19:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:19:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 56.56it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 62.07it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 63.60it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:19:10 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.284 | nll_loss 2.763 | ppl 6.79 | wps 161810 | wpb 2325.1 | bsz 130.5 | num_updates 17133 | best_loss 4.284\n",
      "2024-11-01 15:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 17133 updates\n",
      "2024-11-01 15:19:10 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint17.pt\n",
      "2024-11-01 15:19:11 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint17.pt\n",
      "2024-11-01 15:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint17.pt (epoch 17 @ 17133 updates, score 4.284) (writing took 3.2804835992865264 seconds)\n",
      "2024-11-01 15:19:13 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2024-11-01 15:19:13 | INFO | train | epoch 017 | loss 3.858 | nll_loss 2.407 | ppl 5.3 | wps 61560.2 | ups 17.31 | wpb 3556.3 | bsz 199 | num_updates 17133 | lr 0.000241592 | gnorm 1.028 | loss_scale 16 | train_wall 52 | gb_free 29.7 | wall 1012\n",
      "2024-11-01 15:19:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:19:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 018:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:19:13 | INFO | fairseq.trainer | begin training epoch 18\n",
      "2024-11-01 15:19:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 018: 100%|▉| 1007/1008 [00:54<00:00, 18.66it/s, loss=3.817, nll_loss=2.36,2024-11-01 15:20:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:20:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 53.01it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 60.94it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 62.91it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 66.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:20:08 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.283 | nll_loss 2.753 | ppl 6.74 | wps 160935 | wpb 2325.1 | bsz 130.5 | num_updates 18141 | best_loss 4.283\n",
      "2024-11-01 15:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 18141 updates\n",
      "2024-11-01 15:20:08 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint18.pt\n",
      "2024-11-01 15:20:09 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint18.pt\n",
      "2024-11-01 15:20:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint18.pt (epoch 18 @ 18141 updates, score 4.283) (writing took 3.213758700992912 seconds)\n",
      "2024-11-01 15:20:11 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2024-11-01 15:20:11 | INFO | train | epoch 018 | loss 3.808 | nll_loss 2.349 | ppl 5.09 | wps 61656.3 | ups 17.34 | wpb 3556.3 | bsz 199 | num_updates 18141 | lr 0.000234784 | gnorm 1.031 | loss_scale 16 | train_wall 52 | gb_free 29.9 | wall 1071\n",
      "2024-11-01 15:20:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:20:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 019:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:20:11 | INFO | fairseq.trainer | begin training epoch 19\n",
      "2024-11-01 15:20:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 019: 100%|▉| 1007/1008 [00:54<00:00, 19.38it/s, loss=3.832, nll_loss=2.3762024-11-01 15:21:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:21:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  19%|█▎     | 6/32 [00:00<00:00, 55.41it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  41%|██▍   | 13/32 [00:00<00:00, 61.13it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  62%|███▊  | 20/32 [00:00<00:00, 63.66it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  88%|█████▎| 28/32 [00:00<00:00, 67.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:21:06 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.266 | nll_loss 2.738 | ppl 6.67 | wps 161482 | wpb 2325.1 | bsz 130.5 | num_updates 19149 | best_loss 4.266\n",
      "2024-11-01 15:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 19149 updates\n",
      "2024-11-01 15:21:06 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint19.pt\n",
      "2024-11-01 15:21:07 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint19.pt\n",
      "2024-11-01 15:21:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint19.pt (epoch 19 @ 19149 updates, score 4.266) (writing took 3.2354412600398064 seconds)\n",
      "2024-11-01 15:21:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2024-11-01 15:21:10 | INFO | train | epoch 019 | loss 3.766 | nll_loss 2.301 | ppl 4.93 | wps 61673.9 | ups 17.34 | wpb 3556.3 | bsz 199 | num_updates 19149 | lr 0.000228521 | gnorm 1.037 | loss_scale 32 | train_wall 52 | gb_free 29.8 | wall 1129\n",
      "2024-11-01 15:21:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:21:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1008\n",
      "epoch 020:   0%|                                       | 0/1008 [00:00<?, ?it/s]2024-11-01 15:21:10 | INFO | fairseq.trainer | begin training epoch 20\n",
      "2024-11-01 15:21:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 020: 100%|▉| 1007/1008 [00:54<00:00, 19.04it/s, loss=3.764, nll_loss=2.2982024-11-01 15:22:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2024-11-01 15:22:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|               | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  16%|█      | 5/32 [00:00<00:00, 46.12it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  38%|██▎   | 12/32 [00:00<00:00, 57.24it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  59%|███▌  | 19/32 [00:00<00:00, 58.93it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  84%|█████ | 27/32 [00:00<00:00, 64.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-11-01 15:22:04 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.253 | nll_loss 2.727 | ppl 6.62 | wps 155760 | wpb 2325.1 | bsz 130.5 | num_updates 20157 | best_loss 4.253\n",
      "2024-11-01 15:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 20157 updates\n",
      "2024-11-01 15:22:04 | INFO | fairseq.trainer | Saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint20.pt\n",
      "2024-11-01 15:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to /pfs/data5/home/kit/stud/unyfv/fairseq/checkpoints/checkpoint20.pt\n",
      "2024-11-01 15:22:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint20.pt (epoch 20 @ 20157 updates, score 4.253) (writing took 3.22252644225955 seconds)\n",
      "2024-11-01 15:22:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2024-11-01 15:22:08 | INFO | train | epoch 020 | loss 3.723 | nll_loss 2.252 | ppl 4.76 | wps 61705.1 | ups 17.35 | wpb 3556.3 | bsz 199 | num_updates 20157 | lr 0.000222734 | gnorm 1.039 | loss_scale 32 | train_wall 52 | gb_free 29.7 | wall 1187\n",
      "2024-11-01 15:22:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2024-11-01 15:22:08 | INFO | fairseq_cli.train | done training in 1173.5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make sure that (0.9, 0.98) in \"\", error might occurs when use ''.\n",
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "    binarized_data/iwslt14.de-en \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas \"(0.9, 0.98)\" --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --max-epoch 20 \\\n",
    "    --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate translations with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'binarized_data/iwslt14.de-en', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "INFO:fairseq.tasks.translation:[en] dictionary: 6080 types\n",
      "INFO:fairseq.tasks.translation:[de] dictionary: 7448 types\n",
      "INFO:fairseq_cli.generate:loading model(s) from checkpoints/checkpoint_best.pt\n",
      "/pfs/data5/home/kit/stud/unyfv/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "INFO:fairseq.data.data_utils:loaded 4,179 examples from: binarized_data/iwslt14.de-en/test.en-de.en\n",
      "INFO:fairseq.data.data_utils:loaded 4,179 examples from: binarized_data/iwslt14.de-en/test.en-de.de\n",
      "INFO:fairseq.tasks.translation:binarized_data/iwslt14.de-en test en-de 4179 examples\n",
      "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True\n",
      "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
      "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
      "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n",
      "INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2\n",
      "INFO:fairseq_cli.generate:Translated 4,176 sentences (69,826 tokens) in 10.0s (416.20 sentences/s, 6959.23 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "# ↓↓↓ Even delete this line, it works...xD\n",
    "# TEST_INPUT=\"dataset/spm.tst.de-en.de\"\n",
    "\n",
    "!fairseq-generate binarized_data/iwslt14.de-en \\\n",
    "      --task translation \\\n",
    "      --source-lang en \\\n",
    "      --target-lang de \\\n",
    "      --path checkpoints/checkpoint_best.pt \\\n",
    "      --batch-size 256 \\\n",
    "      --beam 4 \\\n",
    "      --remove-bpe=sentencepiece > \"en-de.decode.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the hypotheses and references from the decoding log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hier ist eine andere Gruppe.\n",
      "Es geht um den Tod.\n",
      "Und ich begann zu fallen.\n",
      "Sie werden es wahrscheinlich nicht.\n",
      "Ich kenne Amira.\n",
      "Stattdessen ist meine Herausforderung das.\n",
      "Es war nur eine Geschichte.\n",
      "Könnte das wahr sein?\n",
      "Dana: So.\n",
      "Wo sind die verschiedenen Länder?\n",
      "\n",
      "Hier ist eine andere Gruppe.\n",
      "Es geht um Tod.\n",
      "Ich begann zu fallen.\n",
      "Sie tun es wahrscheinlich nicht.\n",
      "Ich kenne Amira.\n",
      "Meine Herausforderung besteht eher darin:\n",
      "Es war nur eine Geschichte.\n",
      "Kann das wirklich möglich sein?\n",
      "Dana: Da lang.\n",
      "Ich kann Ihnen Afrika zeigen.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep ^H \"en-de.decode.log\" | sed 's/^H-//g' | cut -f 3 | sed 's/ ##//g' > ./hyp.txt\n",
    "grep ^T \"en-de.decode.log\" | sed 's/^T-//g' | cut -f 2 | sed 's/ ##//g' > ./ref.txt\n",
    "head ./hyp.txt\n",
    "echo \"\"\n",
    "head ./ref.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use BLEU as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 27.1,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\",\n",
      " \"verbose_score\": \"60.5/33.9/21.2/13.6 (BP = 0.977 ratio = 0.977 hyp_len = 52243 ref_len = 53465)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.4.3\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!bash -c \"cat hyp.txt | sacrebleu ref.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
